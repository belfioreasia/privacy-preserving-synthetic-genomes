{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ca64d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import json\n",
    "import random\n",
    "from models.finetuning import *\n",
    "from data.dataset import GPTDataFormatter\n",
    "\n",
    "data_size = 'medium'\n",
    "json_file_path = TRAIN_DATA = f\"/datasets/sources/train_gts_with_pop.json\"\n",
    "HOLDOUT_DATA = f\"/datasets/sources/holdout_gts_with_pop.json\"\n",
    "VAL_DATA = f\"/datasets/sources/val_gts_with_pop.json\"\n",
    "model_name = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc320e0-f610-4763-baa4-d0267ce82a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3222e70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1. Pre-trained GPT from HF for benchmarking (No fine tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b160e206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(json_file_path, 'r') as f:\n",
    "    original_genotypes = json.load(f)\n",
    "\n",
    "sample_ids = list(original_genotypes.keys())\n",
    "chosen_sample_id = random.choice(sample_ids)\n",
    "\n",
    "prompt = original_genotypes[chosen_sample_id]['genotypes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c6b5cf0-7cfe-4d80-ae0a-17108b8bb7ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22:16056839:C>T_0|0\n"
     ]
    }
   ],
   "source": [
    "print(prompt[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bab854d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample ID: synth_1\n",
      "Generated Genotype:  22:16056839:C>T_0|0|r31|4145|Beth|d[5]1[1]|0|r31|4143|Terrace Wiccan - Do You (feat. T.J. Cole)|4.0|0|2 3646|Dirty Mustard - N.Y.D.I.P. (feat. Pharrell Williams)|4.0|0|1 3646|Frank Ocean - The End of The World - 2008 Remastered Version|4.0|0|3 3646|Drake - I Need You (feat. Lil Uzi Vert)|4.0|0|2 3646|Barry Jackson - I Feel Lucky|4.0|0|2 3646|Drake - No Good|4.0|0|2 3646|Drake - No Love|4.0|0|2 3646|Drake - Never Fade Away|4.0|0|2 3646|Drake - Red|4.0|0|1 3646|Daniel Caesar - Love|4.0|0|1 3646|danielcalabo - The Heart-Lust of It|4.0|0|1 3646|Doink - All I Want|4.0|0|1 3646|Dirty Jeezy - We're All Sober|4.0|0|1 3646|Denzel Curry - The Road|4.0|0|1 3646|Denzel Curry - We Love You|4.0|0|1 3646|Denzel Curry - I'm Waiting|4.0|0|1 3646|Denzel Curry - My God|4.0|0|1 3646|Denzel Curry - Next Love|4.0|0|1 3646|Daft Punk - Sober|4.0|0|1 3646|Daft Punk - I'm the Most Beautiful Man|4.0|0|1 3646|FKA twigs - P.D.O.S.I.D.|4.0|0|3 3646|FKA twigs - Stay On My Own|4.0|0|3 3646|Florence + the Machine - Love (feat. Pharrell Williams)|4.0|\n",
      "\n",
      "Sample ID: synth_2\n",
      "Generated Genotype:  22:16056839:C>T_0|0|2 362|dino - The Night To Remember|12.0|0|1 362|Bassnectar - Backwards|12.0|0|1 362|Lil B - Blue (feat. Jazmine Sullivan)|12.0|0|1 362|Miley Cyrus - All The Time|12.0|0|1 362|Britney Spears - Call Me Maybe|12.0|0|1 362|Demi Lovato - Don't Be Fooled by The World|12.0|0|1 362|B.o.B - The Way You Sleep|12.0|0|1 362|Karaoke - Pucker|12.0|0|1 362|BjÃ¶rk - Take a Chance|12.0|0|1 362|Katy Perry - All I Need|12.0|0|1 362|LIL B - I Just Think I'm Sexy|12.0|0|1 362|Bruno Mars and The Heartbreakers - Hold on, Hold On/TALK: Live|12.0|0|1 362|Bruno Mars - The Real Love Song|12.0|0|1 362|Katy Perry - Red Is For Grown Up|12.0|0|1 362|Taylor Swift - Like a Prayer|12.0|0|1 362|Kodak Black - The Lament|12.0|0|1 362|Zedd - Never Get in Your Way Again|12.0|0|1 362|Lady Gaga - Let Her Go|12.0|0|1 362|Muse - In My Life|12.0|0|1 362|Jenny White - I'll Tell You I Hate It (feat. Ariana Grande)|12.0|0|1 362|BjÃ¶rk - I Just Want You To Go Home|12.0|0|1 362|Kanye West - Run|12.0|0|1 362|Kanye West - Run On A Dream|12.0|0|1 362|Jaden Smith - Don't Be Like This|12.0|0|1 362\n",
      "\n",
      "Sample ID: synth_3\n",
      "Generated Genotype:  22:16056839:C>T_0|0|1 3315|Taylor Swift - Do Ya?|15:17074923.36|0|1 3332|Taylor Swift - Take You All Home|15:17034811.56|0|4 3332|Sufjan Stevens - You're My Sunshine - Remastered (feat. Taylor Swift)|15:17014918.43|0|1 3332|Sufjan Stevens - I Don't Want You With Me|15:17014925.23|0|1 3332|Sufjan Stevens - I Can't Take it Like This|15:17014942.24|0|1 3332|Sufjan Stevens - It's Okay to Be Like What You Are|15:17014942.43|0|1 3332|The Chainsmokers - You Know Why You Love Me So Much|15:17014944.59|0|3 3332|The Chainsmokers - You're My Sunshine|15:17017503.22|0|4 3332|The Chainsmokers - Just A Memory - 2007 Remastered Version|15:17949043.25|0|4 3332|The Chainsmokers - Nothing's the Same|15:17164549.25|0|3 3332|The Chainsmokers - Just A Memory - 2009 Remastered Version|15:17949046.38|0|3 3332|The Chainsmokers - Just A Memory - 2007 Remastered Version|15:17949058.23|0|2 3332|The Chainsmokers - Hey Ya|15:17165448.58|0|2 3332|The Chainsmokers - High and Dry|15:17164825.08|0|3 3332|The Chainsmokers - Nothing But Love|15:17164826.03|0|2 3332|The Chainsmokers - Old Days Will Come and Gone|15:17166648.22|0-Recording|15:17165660.43|0|3 3332|Rae Sremmurd - We'll Always Be Like This|15:17168699.33|0|2 3332|Rae Sremmurd - To You|15:17168699.33\n",
      "\n",
      "Sample ID: synth_4\n",
      "Generated Genotype:  22:16056839:C>T_0|0|1 33334|Vodafone - No One's Fault|4.5|0|1 33334|Porter Robinson - Fond of Love|4.5|0|1 33334|Charli XCX - I'm So Happy And I Love It|4.5|0|1 33334|BROCKHAMPTON - Bitter and Sweeter|4.5|0|1 33334|BROCKHAMPTON - BREATHING|4.5|0|1 33334|Tinashe - I Love You Like Diamond (feat. Tame Impala)|4.5|0|1 33334|BROCKHAMPTON - CRIMINALIZING/SCREENSHOTS/DUAL|4.5|0|1 33334|Demi Lovato - Bitch Too|4.5|0|1 33334|Charli XCX - This Is Something I Think Is Just Not Happening (feat. XXXTENTACION)|4.5|0|1 33334|Rihanna - I'm a Girl|4.5|0|1 33334|BROCKHAMPTON - I Gotta Go Home|4.5|0|1 33334|Jynx - A-P-F|4.5|0|1 33334|RZA - I'm a Girl (feat. Rihanna)|4.5|0|1 33334|Tinashe - Cry Me a River|4.5|0|1 33334|Calvin Harris - I Don't Mind You (feat. R. Kelly)|4.5|0|1 33334|RZA - Take it or Leave It|4.5|0|1 33334|Ariana Grande & Gucci Mane - Aint Nothing|4.5|0|1 33334|Kendrick Lamar - Superb|4.5|0|1 33334|Kanye West - My Girl I Like - Instrumental Version|4.5|0|1 33334|Rae Sremmurd - Back|4.5|0|1 33334|Stacey K - So Happy You're Ready|4.5|0|1 33334|Charli XCX - Can't Hold My Love|4\n",
      "\n",
      "Sample ID: synth_5\n",
      "Generated Genotype:  22:16056839:C>T_0|0|2 2814|Taylor Swift - The Promised Land (feat. T.I)|14.5|0|1 2910|Dastardly - The Night I Stay|14.5|0|1 2910|David Wilson - Bad Moods|14.5|0|1 2910|Purity Ring - Love Can Come|14.5|0|1 2910|Kanye West - The Future (feat. Ariana Grande) (feat. Kendrick Lamar)|14.5|0|1 2910|Britney Spears - Love Is A Lie|14.5|0|1 2910|Kehlani - Get Lucky|14.5|0|1 2910|Ed Sheeran - The Girl Who Died in My Arms - Live With Kendrick Lamar On \"Jellyfish Ballroom\" On \"Yeezus,\" Recorded in Toronto, ON June 16, 2015 (All Songs First)|14.5|0|1 2910|AJ - Beautiful Life|14.5|0|1 2910|A$AP Mob - I'm Not Ready|14.5|0|1 2910|Annie Lennox - Just Like Heaven|14.5|0|1 2910|Avril Lavigne - It's Only Natural|14.5|0|1 2910|Britney Spears - Myself|14.5|0|1 2910|Kanye West - Like a Wolf|14.5|0|1 2910|Pimp Cinco - Cope My Own Way|14.5|0|1 2910|Halsey - No More|14.5|0|3 3017|A$AP Mob - U U Got Some Crazy Songs|14.0|0|4 3017|Halsey - All I Need|14.0|0|4 3017|Frank Ocean - Just a Basket of Bones|14.0|0|3 3017|MCA - Gonna Make Up (feat. Lil Wayne)|14.0|0|5 3017|Allie X - Don't Be Sad|14.0|0|3 3017|Kanye West - The Voice of the Year (feat. Wiz Khalifa)|14.0|0|5 3017|Katy Perry -\n",
      "\n",
      "Sample ID: synth_6\n",
      "Generated Genotype:  22:16056839:C>T_0|0|1 23:16587626:C>T_0|0|1 23:16783843:C>T_0|0|2 23:16783973:C>T_0|0|3 23:16783866:C>T_0|0|4 23:16783958:C>T_0|0|5 23:16783876:C>T_0|0|6 23:16783987:C>T_0|0|7 23:16783891:C>T_0|0|8 23:16784227:C>T_0|0|9 23:16783664:C>T_0|0|10 23:1678368:C>T_0|0|11 23:16783862:C>T_0|0|12 23:1678379:C>T_0|0|13 23:16783893:C>T_0|0|14 23:16777373:C>T_0|0|15 23:16780334:C>T_0|0|16 23:16783958:C>T_0|0|17 23:17027383:C>T_0|0|18 23:17134947:C>T_0|0|19 23:17143314:C>T_0|0|20 23:17139525:C>T_0|0|21 23:17138735:C>T_0|0|22 23:17138541:C>T_0|0|23 23:17139045:C>T_0|0|24 23:17139525:G>T_0|0|25 23:17138908:G>T_0|0|26 23:17144067:G>T_0|0|27 23:17144077:G>T_0|0|28 23:17144078:G>T_0|0|29 23:17144111:G>T_0|0|30 23:17144089:G>T\n",
      "\n",
      "Sample ID: synth_7\n",
      "Generated Genotype:  22:16056839:C>T_0|0|1 3523.680069:WAD|r|p 2252.522939:DAT|w|t|w - 1.05% 26:46.758048:GIN|r|p 1913.723322:WAD|r|p 722.522939:DAT|w|t|w -1.05% 26:46.788048:GIN|r|p 737.522939:DAT|w|t|w -1.05% 26:46.788048:GIN|r|p 764.522939:DAT|w|t|w -1.05% 26:46.793937:GIN|r|p 928.723322:WAD|r|p 795.758048:DAT|w|t|s -1.03% 26:46.793937:GIN|r|p 936.723322:WAD|r|p 796.758048:DAT|w|t|s -1.03% 26:46.796999.0|r|p 777.823223:WAD|r|p 822.758048:DAT|w|t|s -1.03% 26:46.82733.4|r|p 812.758048:DAT|w|t|s -1.03% 26:46.85834.0|r|p 816.758048:DAT|w|t|s -1.03% 26:46.862675.1|r|p 826.753048:DAG|w|t|w -1.03% 26:46.863575.4|r|p 838.758048:DAG|w|t|w -1.03% 26:46.846425.0|r|p 854.758048:DAG|w|t|s -1.03% 26:46.97914.0|r|p 909.522322:DAG|h|p 742.680149:\n",
      "\n",
      "Sample ID: synth_8\n",
      "Generated Genotype:  22:16056839:C>T_0|0|1 4325|Naglfar - Do it (feat. J-Hope)|1.0|0|1 4325|Nat King Cole - I Love You|1.0|0|1 4325|The Killers - My Dear|1.0|0|1 4325|The Killers - So Help Me God|1.0|0|1 4325|T-Pain - Feelin' Love|1.0|0|1 4325|The Killers - Still Alive|1.0|0|1 4325|The Killers - Still Alive|1.0|0|1 4325|Vince Staples - Hey Baby, Don't Forget to Come Right Now|1.0|0|1 4325|Vince Staples - Come And Give It Back to Me|1.0|0|1 4325|Vince Staples - Go Home|1.0|0|1 4325|Victoria - Suck Me|1.0|0|1 4325|Victoria - Stay|1.0|0|1 4325|Vince Staples - You Got a Hard Drive|1.0|0|1 4325|Wishbone - We Will Fall|1.0|0|1 4325|Wishbone - You Should Know|1.0|0|1 4325|Wishbone - Yellow|1.0|0|1 4325|Zina Sombra - Wanna Be Friends With Me|1.0|0|1 4325|Zizzi - Good|1.0|0|1 4325|Zara Larsson - Dreamin'|1.0|0|1 4325|Zinnia - Don't Be Fooled By My Dreams|1.0|0|1 4325|Zona Halep - You Feel Better|1.0|0|1 4325|Zara Larsson - The Real Enemy|1.0|0|1 4325|Zara Larsson - Yellow Lights|1.0|0|1 4325|Zara Larsson - Where Is My Love|1.0|0|1 4325|Zara Larsson - Yellow Sun|1.0|0|1 4325|Zara Larsson - Yellow Day|1\n",
      "\n",
      "Sample ID: synth_9\n",
      "Generated Genotype:  22:16056839:C>T_0|0|1 3330|Chris Brown - Get On Over Me|1.0|0|1 3295|E-40 - We're Never Ever Getting Over Me|1.0|0|1 3295|Justin Timberlake - My Lovesong Face|1.0|0|1 3295|Drake - In Between Now and Now|1.0|0|1 3295|Lana Del Rey - You|1.0|0|1 3295|Fandango - Get Me Home|1.0|0|1 3295|Ariana Grande - L.A. Times|1.0|0|1 3295|Kanye West - No Good Dealing With What Feelings|1.0|0|1 3295|Sufjan Stevens - Bitch|1.0|0|1 3295|Halsey - Dope|1.0|0|1 3295|Drake - Gonna Be|1.0|0|1 3295|Jay-Z - Blackstar|1.0|0|1 3295|Dirty Projectors - Come Out Tonight|1.0|0|1 3295|Tove Lo - Pity|1.0|0|1 3295|Katy Perry - Dope|1.0|0|1 3295|Lights - Get Over Me|1.0|0|1 3295|Kanye West - All In|1.0|0|1 3295|Kanye West - Back In This World|1.0|0|1 3295|The Weeknd - Like You|1.0|0|1 3295|Lil Ugly Dick - Bitch (feat. A$AP Rocky & Drake)|1.0|0|1 3295|BeyoncÃ© - Get Up With Me|1.0|0|1 3295|Kelly Rowland - Love You|1.0|0|1 3295|Madonna - When The Time Calls|1.0|0|1 3295|Lady Gaga - How Did You Know|1.0|0|1 3295|Drake - Come To The Ballad|1.0|0|1 3295|Kelela - Let's Live It Down (feat. Ari\n",
      "\n",
      "Sample ID: synth_10\n",
      "Generated Genotype:  22:16056839:C>T_0|0|1 3334|Monsanto|11.13.2006|5.62|0|4 3646|Harlan Ellison - Run-By-Groups|11.13.2006|5.54|0|3 3646|Shawn Mendes - In Memory of The City|11.13.2006|5.47|0|3 3646|A$AP Ferg - A Conversation|11.13.2006|5.42|0|3 3646|Allum Bokhari - Hijab on the Bus|11.13.2006|5.20|0|3 3646|W.E.B - No Woman|11.13.2006|5.18|0|3 3646|The Chainsmokers - The Man In The Window For You|11.13.2006|4.96|0|3 3646|Nicki Minaj - Love Is More Pure Than Words (feat. Rihanna)|11.13.2006|4.68|0|3 3646|Justin Timberlake - Hold On (feat. JAY Z)|11.13.2006|4.45|0|3 3646|R. Kelly - Run On The Wind|11.13.2006|4.39|0|3 3646|Demi Lovato - The Dance Of Death|11.13.2006|4.39|0|3 3646|Vince Staples - Pardon My French|11.13.2006|4.29|0|3 3646|Drake - Come on, It's The Sun on Thaw|11.13.2006|3.88|0|2 2200|Julius Clay - Let It Go in The House|11.13.2006|3.86|0|2 2200|St. Vincent - Sucks On You (Remix)|11.13.2006|3.84|0|2 2200|Shania Twain - Love Can Keep On Gently|11.13.2006|3.75|0|2 2200|The Tylenol Park Band - The Good Life|11.13.2006|3.64|0|2 2200|Vince Staples (The Strokes and The Roots' Superfluid Boys) & M.I.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "prompt_length = 20\n",
    "num_muts = 500\n",
    "\n",
    "pretrained_model = PretrainedGPT(model_name=model_name, device='cpu')\n",
    "benchmark_samples = pretrained_model.generate(prompt[:prompt_length], \n",
    "                                              samples_to_generate=num_samples, \n",
    "                                              max_length=num_muts, \n",
    "                                              save_path=f'baseline_gpt2_pretrained.json')\n",
    "for sample_id, gt in benchmark_samples.items():\n",
    "    print(f\"Sample ID: {sample_id}\\nGenerated Genotype: {gt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b1864",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35dd61c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Finetuning GPT from HF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ae05c-4ead-40b4-bbeb-30492e3d5680",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.A Without DP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a20a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.a Finetuning/Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8da37af-8979-4cdb-85b2-091973d48a3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T12:12:30.031440Z",
     "iopub.status.busy": "2025-08-30T12:12:30.030777Z",
     "iopub.status.idle": "2025-08-30T12:12:35.521567Z",
     "shell.execute_reply": "2025-08-30T12:12:35.518421Z",
     "shell.execute_reply.started": "2025-08-30T12:12:30.031440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading and formatting dataset...\n",
      "   Loaded 2504 samples\n",
      "   Formatted 2504 sample sequences\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "formatter = GPTDataFormatter()\n",
    "\n",
    "print(\"1. Loading and formatting dataset...\")\n",
    "# Load data\n",
    "original_genotypes = formatter.load_data_from_json(json_file_path)\n",
    "print(f\"   Loaded {len(original_genotypes)} samples\")\n",
    "# Format data\n",
    "formatted_genotype_seqs = formatter.get_training_corpus(original_genotypes)\n",
    "print(f\"   Formatted {len(formatted_genotype_seqs)} sample sequences\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eae4f26e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T12:12:36.915322Z",
     "iopub.status.busy": "2025-08-30T12:12:36.914586Z",
     "iopub.status.idle": "2025-08-30T12:12:40.203741Z",
     "shell.execute_reply": "2025-08-30T12:12:40.200432Z",
     "shell.execute_reply.started": "2025-08-30T12:12:36.915290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Setting up model and tokenizer...\n",
      "Running on cuda\n",
      "   Model vocabulary size: 50266\n",
      "3. Preparing training pipeline...\n",
      "   Training samples: 1752\n",
      "   Evaluation samples: 752\n"
     ]
    }
   ],
   "source": [
    "print(\"2. Setting up model and tokenizer...\")\n",
    "# Setup model and tokenizer\n",
    "# Initialize trainer\n",
    "trainer = FinetuningTrainer(model_name=model_name,\n",
    "                special_tokens=formatter.special_tokens)\n",
    "print(f\"   Model vocabulary size: {len(trainer.tokenizer)}\")\n",
    "\n",
    "print(\"3. Preparing training pipeline...\")\n",
    "# Prepare datasets\n",
    "train_dataset, eval_dataset = trainer.setup_training_data(formatted_genotype_seqs)\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Evaluation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b6b1462-3091-4f43-89a0-def46c500b7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T12:12:46.483912Z",
     "iopub.status.busy": "2025-08-30T12:12:46.482412Z",
     "iopub.status.idle": "2025-08-30T12:48:18.303348Z",
     "shell.execute_reply": "2025-08-30T12:48:18.301805Z",
     "shell.execute_reply.started": "2025-08-30T12:12:46.483912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Training model...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbelfiore-asia\u001b[0m (\u001b[33mbelfiore-asia-imperial-college-london\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20250830_121249-08aev9u4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/belfiore-asia-imperial-college-london/huggingface/runs/08aev9u4' target=\"_blank\">zesty-brook-14</a></strong> to <a href='https://wandb.ai/belfiore-asia-imperial-college-london/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/belfiore-asia-imperial-college-london/huggingface' target=\"_blank\">https://wandb.ai/belfiore-asia-imperial-college-london/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/belfiore-asia-imperial-college-london/huggingface/runs/08aev9u4' target=\"_blank\">https://wandb.ai/belfiore-asia-imperial-college-london/huggingface/runs/08aev9u4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='657' max='657' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [657/657 32:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>8.362100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.042800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Evaluating model...\n",
      "Evaluating model..."
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 02:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "  eval_loss: 0.001834377646446228\n",
      "  eval_runtime: 143.1306\n",
      "  eval_samples_per_second: 5.254\n",
      "  eval_steps_per_second: 1.313\n",
      "  epoch: 3.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1493b27d34346aaa349e5c0b56b7277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.018 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.106876â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â</td></tr><tr><td>eval/runtime</td><td>â</td></tr><tr><td>eval/samples_per_second</td><td>â</td></tr><tr><td>eval/steps_per_second</td><td>â</td></tr><tr><td>train/epoch</td><td>ââââââââ</td></tr><tr><td>train/global_step</td><td>ââââââââ</td></tr><tr><td>train/learning_rate</td><td>ââââââ</td></tr><tr><td>train/loss</td><td>ââââââ</td></tr><tr><td>train/total_flos</td><td>â</td></tr><tr><td>train/train_loss</td><td>â</td></tr><tr><td>train/train_runtime</td><td>â</td></tr><tr><td>train/train_samples_per_second</td><td>â</td></tr><tr><td>train/train_steps_per_second</td><td>â</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.00183</td></tr><tr><td>eval/runtime</td><td>143.1306</td></tr><tr><td>eval/samples_per_second</td><td>5.254</td></tr><tr><td>eval/steps_per_second</td><td>1.313</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>657</td></tr><tr><td>train/learning_rate</td><td>0.0004</td></tr><tr><td>train/loss</td><td>0.0032</td></tr><tr><td>train/total_flos</td><td>2682326016000000.0</td></tr><tr><td>train/train_loss</td><td>1.28518</td></tr><tr><td>train/train_runtime</td><td>1979.8257</td></tr><tr><td>train/train_samples_per_second</td><td>2.655</td></tr><tr><td>train/train_steps_per_second</td><td>0.332</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-brook-14</strong> at: <a href='https://wandb.ai/belfiore-asia-imperial-college-london/huggingface/runs/08aev9u4' target=\"_blank\">https://wandb.ai/belfiore-asia-imperial-college-london/huggingface/runs/08aev9u4</a><br/> View job at <a href='https://wandb.ai/belfiore-asia-imperial-college-london/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjcyODM5ODk3OQ==/version_details/v2' target=\"_blank\">https://wandb.ai/belfiore-asia-imperial-college-london/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjcyODM5ODk3OQ==/version_details/v2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250830_121249-08aev9u4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"4. Training model...\")\n",
    "# Setup training arguments\n",
    "batch_size = 4 # higher gives CUDA out of memory errors on my device\n",
    "training_args = trainer.setup_trainer(\n",
    "    epochs=3,\n",
    "    train_batch_size=batch_size,\n",
    "    eval_batch_size=batch_size,\n",
    "    learning_rate=1e-3)\n",
    "\n",
    "# Train the model\n",
    "trained_trainer = trainer.train_model(train_dataset, eval_dataset, training_args)\n",
    "\n",
    "print(\"5. Evaluating model...\")\n",
    "# Evaluate\n",
    "eval_results = trainer.evaluate_model(trained_trainer)\n",
    "\n",
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b0079ec-b2b7-412e-9e16-ac5c263a2935",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T13:02:35.433940Z",
     "iopub.status.busy": "2025-08-30T13:02:35.432951Z",
     "iopub.status.idle": "2025-08-30T13:02:37.095189Z",
     "shell.execute_reply": "2025-08-30T13:02:37.093898Z",
     "shell.execute_reply.started": "2025-08-30T13:02:35.433892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22:16056839:C>T_0|0\n"
     ]
    }
   ],
   "source": [
    "with open(\"/datasets/sources/genotypes_medium_chr22.json\", 'r') as f:\n",
    "    original_genotypes = json.load(f)\n",
    "\n",
    "sample_ids = list(original_genotypes.keys())\n",
    "chosen_sample_id = random.choice(sample_ids)\n",
    "\n",
    "prompt = original_genotypes[chosen_sample_id]['genotypes'][:20]\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c98f0ac-f632-48be-b878-3c57ff93285b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T13:29:30.736841Z",
     "iopub.status.busy": "2025-08-30T13:29:30.736447Z",
     "iopub.status.idle": "2025-08-30T13:29:43.926380Z",
     "shell.execute_reply": "2025-08-30T13:29:43.925283Z",
     "shell.execute_reply.started": "2025-08-30T13:29:30.736814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. Generating sample outputs...\n",
      "Generating on cuda\n",
      "   synth_1: 22:16163523:T>A_0|0 22:16185747:T>C_0|0 22:16197860:A>G_0|0 22:16202129:C>T_0|0 22:16223429:A>T_0|0 22:16237892:A>G_0|0 22:16239684:G>A_0|0 22:16244406:A>C_0|0 22:16265087:T>C_0|0 22:16268948:G>C_0|0 22:16300070:C>T_0|0 22:16336692:G>T_0|0 22:16340011:C>T_0|0 22:16341823:C>T_0|0 22:16346577:A>G_0|0 22:16353763:G>A_0|0  ...\n"
     ]
    }
   ],
   "source": [
    "print(\"6. Generating sample outputs...\")\n",
    "# Generate some samples to test\n",
    "samples = generate_sample(model=trainer.model,\n",
    "                        tokenizer=trainer.tokenizer,\n",
    "                        formatter=formatter,\n",
    "                        samples_to_generate=1,\n",
    "                        max_sample_length=500,\n",
    "                        prompt=prompt,\n",
    "                        return_tensors=False,)\n",
    "\n",
    "generated_samples = {f'synth_{i+1}':sample for i, sample in enumerate(samples)}\n",
    "    \n",
    "for idx, genotype in generated_samples.items():\n",
    "    print(f\"   {idx}: {genotype[200:]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff2f26",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae5e642",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.b Inference from saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e021b0b8-ee45-499a-b9fa-03010f67c1eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T13:26:06.232392Z",
     "iopub.status.busy": "2025-08-30T13:26:06.231790Z",
     "iopub.status.idle": "2025-08-30T13:26:16.029634Z",
     "shell.execute_reply": "2025-08-30T13:26:16.028793Z",
     "shell.execute_reply.started": "2025-08-30T13:26:06.232365Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-30 13:26:10.444820: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-30 13:26:10.444895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-30 13:26:10.446453: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-30 13:26:10.456450: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-30 13:26:11.836090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import json\n",
    "import random\n",
    "from models.finetuning import *\n",
    "from data.dataset import GPTDataFormatter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "output_dir = \"models/saved/GPT\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a68ba6-70fc-4187-b5fe-1024463376b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T13:28:51.097234Z",
     "iopub.status.busy": "2025-08-30T13:28:51.096856Z",
     "iopub.status.idle": "2025-08-30T13:28:56.070227Z",
     "shell.execute_reply": "2025-08-30T13:28:56.069439Z",
     "shell.execute_reply.started": "2025-08-30T13:28:51.097208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "formatter = GPTDataFormatter()\n",
    "trainer = FinetuningTrainer(model_name='gpt2',\n",
    "                special_tokens=formatter.special_tokens)\n",
    "trainer.tokenizer = tokenizer\n",
    "trainer.model = model\n",
    "\n",
    "with open(\"/datasets/sources/genotypes_medium_chr22.json\", 'r') as f:\n",
    "    original_genotypes = json.load(f)\n",
    "\n",
    "sample_ids = list(original_genotypes.keys())\n",
    "chosen_sample_id = random.choice(sample_ids)\n",
    "\n",
    "prompt = original_genotypes[chosen_sample_id]['genotypes'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0e13f76-e3cc-433d-9908-749132f49c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T13:29:55.066183Z",
     "iopub.status.busy": "2025-08-30T13:29:55.065405Z",
     "iopub.status.idle": "2025-08-30T13:30:13.602302Z",
     "shell.execute_reply": "2025-08-30T13:30:13.601016Z",
     "shell.execute_reply.started": "2025-08-30T13:29:55.066146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample outputs...\n",
      "Generating on cuda\n",
      "   synth_1: 22:16163523:T>A_0|0 22:16185747:T>C_0|0 22:16197860:A>G_0|0 22:16202129:C>T_0|0 22:16223429:A>T_0|0 22:16237892:A>G_0|0 22:16239684:G>A_0|0 22:16244406:A>C_0|0 22:16265087:T>C_0|0 22:16268948:G>C_0|0 22:16300070:C>T_0|0 22:16336692:G>T_0|0 22:16340011:C>T_0|0 22:16341823:C>T_0|0 22:16346577:A>G_0|0 22:16353763:G>A_0|0  ...\n",
      "   synth_2: 22:16163523:T>A_0|0 22:16185747:T>C_0|0 22:16197860:A>G_0|0 22:16202129:C>T_0|0 22:16223429:A>T_0|0 22:16237892:A>G_0|0 22:16239684:G>A_0|0 22:16244406:A>C_0|0 22:16265087:T>C_0|0 22:16268948:G>C_0|0 22:16300070:C>T_0|0 22:16336692:G>T_0|0 22:16340011:C>T_0|0 22:16341823:C>T_0|0 22:16346577:A>G_0|0 22:16353763:G>A_0|0  ...\n",
      "   synth_3: 22:16163523:T>A_0|0 22:16185747:T>C_0|0 22:16197860:A>G_0|0 22:16202129:C>T_0|0 22:16223429:A>T_0|0 22:16237892:A>G_0|0 22:16239684:G>A_0|0 22:16244406:A>C_0|0 22:16265087:T>C_0|0 22:16268948:G>C_0|0 22:16300070:C>T_0|0 22:16336692:G>T_0|0 22:16340011:C>T_0|0 22:16341823:C>T_0|0 22:16346577:A>G_0|0 22:16353763:G>A_0|0  ...\n",
      "   synth_4: 22:16163523:T>A_0|0 22:16185747:T>C_0|0 22:16197860:A>G_0|0 22:16202129:C>T_0|0 22:16223429:A>T_0|0 22:16237892:A>G_0|0 22:16239684:G>A_0|0 22:16244406:A>C_0|0 22:16265087:T>C_0|0 22:16268948:G>C_0|0 22:16300070:C>T_0|0 22:16336692:G>T_0|0 22:16340011:C>T_0|0 22:16341823:C>T_0|0 22:16346577:A>G_0|0 22:16353763:G>A_0|0  ...\n",
      "   synth_5: 22:16163523:T>A_0|0 22:16185747:T>C_0|0 22:16197860:A>G_0|0 22:16202129:C>T_0|0 22:16223429:A>T_0|0 22:16237892:A>G_0|0 22:16239684:G>A_0|0 22:16244406:A>C_0|0 22:16265087:T>C_0|0 22:16268948:G>C_0|0 22:16300070:C>T_0|0 22:16336692:G>T_0|0 22:16340011:C>T_0|0 22:16341823:C>T_0|0 22:16346577:A>G_0|0 22:16353763:G>A_0|0  ...\n",
      "   synth_6: 22:16163523:T>A_0|0 22:16185747:T>C_0|0 22:16197860:A>G_0|0 22:16202129:C>T_0|0 22:16223429:A>T_0|0 22:16237892:A>G_0|0 22:16239684:G>A_0|0 22:16244406:A>C_0|0 22:16265087:T>C_0|0 22:16268948:G>C_0|0 22:16300070:C>T_0|0 22:16336692:G>T_0|0 22:16340011:C>T_0|0 22:16341823:C>T_0|0 22:16346577:A>G_0|0 22:16353763:G>A_0|0  ...\n",
      "   synth_7: 22:16163523:T>A_0|0 22:16185747:T>C_0|0 22:16197860:A>G_0|0 22:16202129:C>T_0|0 22:16223429:A>T_0|0 22:16237892:A>G_0|0 22:16239684:G>A_0|0 22:16244406:A>C_0|0 22:16265087:T>C_0|0 22:16268948:G>C_0|0 22:16300070:C>T_0|0 22:16336692:G>T_0|0 22:16340011:C>T_0|0 22:16341823:C>T_0|0 22:16346577:A>G_0|0 22:16353763:G>A_0|0  ...\n",
      "   synth_8: 22:16163523:T>A_0|0 22:16185747:T>C_0|0 22:16197860:A>G_0|0 22:16202129:C>T_0|0 22:16223429:A>T_0|0 22:16237892:A>G_0|0 22:16239684:G>A_0|0 22:16244406:A>C_0|0 22:16265087:T>C_0|0 22:16268948:G>C_0|0 22:16300070:C>T_0|0 22:16336692:G>T_0|0 22:16340011:C>T_0|0 22:16341823:C>T_0|0 22:16346577:A>G_0|0 22:16353763:G>A_0|0  ...\n",
      "   synth_9: 22:16163523:T>A_0|0 22:16185747:T>C_0|0 22:16197860:A>G_0|0 22:16202129:C>T_0|0 22:16223429:A>T_0|0 22:16237892:A>G_0|0 22:16239684:G>A_0|0 22:16244406:A>C_0|0 22:16265087:T>C_0|0 22:16268948:G>C_0|0 22:16300070:C>T_0|0 22:16336692:G>T_0|0 22:16340011:C>T_0|0 22:16341823:C>T_0|0 22:16346577:A>G_0|0 22:16353763:G>A_0|0  ...\n",
      "   synth_10: 22:16163523:T>A_0|0 22:16185747:T>C_0|0 22:16197860:A>G_0|0 22:16202129:C>T_0|0 22:16223429:A>T_0|0 22:16237892:A>G_0|0 22:16239684:G>A_0|0 22:16244406:A>C_0|0 22:16265087:T>C_0|0 22:16268948:G>C_0|0 22:16300070:C>T_0|0 22:16336692:G>T_0|0 22:16340011:C>T_0|0 22:16341823:C>T_0|0 22:16346577:A>G_0|0 22:16353763:G>A_0|0  ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating sample outputs...\")\n",
    "# Generate some samples to test\n",
    "samples = generate_sample(model=trainer.model,\n",
    "                        tokenizer=trainer.tokenizer,\n",
    "                        formatter=formatter,\n",
    "                        samples_to_generate=10,\n",
    "                        max_sample_length=500,\n",
    "                        prompt=prompt,\n",
    "                        return_tensors=False,)\n",
    "\n",
    "generated_samples = {f'synth_{i+1}':sample for i, sample in enumerate(samples)}\n",
    "with open('finetuned_gpt2_10samples.json', 'w+') as f:\n",
    "    json.dump(generated_samples, f, sort_keys=False, indent=4)\n",
    "    \n",
    "for idx, genotype in generated_samples.items():\n",
    "    print(f\"   {idx}: {genotype[200:]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86474ae2-12d2-413e-a526-bb15839f68a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.A.2 Without DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e95c47-5e85-4cfe-8156-984464d45298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import models.finetuning\n",
    "reload(models.finetuning)\n",
    "from models.finetuning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc5a272-2bdc-4890-a82f-a2816b2baf26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and formatting dataset...\n",
      "   Loaded 1752 samples\n",
      "   Formatted 1752 sample sequences\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "formatter = GPTDataFormatter(custom=True)\n",
    "\n",
    "print(\"Loading and formatting dataset...\")\n",
    "# Load data\n",
    "original_genotypes = formatter.load_data_from_json(json_file_path)\n",
    "print(f\"   Loaded {len(original_genotypes)} samples\")\n",
    "# Format data\n",
    "formatted_genotype_seqs = formatter.get_training_corpus(original_genotypes)\n",
    "print(f\"   Formatted {len(formatted_genotype_seqs)} sample sequences\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e742b961-0f70-412e-aaef-2cefa207552d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model and tokenizer:\n",
      "Running on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up model and tokenizer:\")\n",
    "# Setup model and tokenizer\n",
    "# Initialize trainer\n",
    "trainer = FinetuningTrainer(model_name='gpt2',\n",
    "                            special_tokens=formatter.special_tokens,\n",
    "                            use_privacy = False)\n",
    "# print(f\"   Model vocabulary size: {len(trainer.tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a205b79-6f70-4c5e-bb84-bccfc3edfae8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training pipeline:\n",
      "   Training samples: 1226\n",
      "   Evaluation samples: 526\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing training pipeline:\")\n",
    "# Prepare datasets\n",
    "train_dataset, eval_dataset = trainer.setup_training_data(formatted_genotype_seqs)\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Evaluation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1db2ee87-1760-4bc7-ae17-2682ac2a7082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c786ab3-ec12-487f-bab1-7a1233243ffa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.00 GB\n",
      "Total reserved memory: 0.00 GB\n",
      "Free memory: 15.88 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "allocated_memory = torch.cuda.memory_allocated()\n",
    "print(f\"Allocated memory: {allocated_memory / (1024 ** 3):.2f} GB\")\n",
    "total_memory = torch.cuda.memory_reserved()\n",
    "print(f\"Total reserved memory: {total_memory / (1024 ** 3):.2f} GB\")\n",
    "free_memory = torch.cuda.memory_allocated()\n",
    "print(f\"Free memory: {(torch.cuda.get_device_properties(0).total_memory - free_memory) / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1ee8f8f-8614-48bb-956f-212f44ac252f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1226\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='380' max='380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [380/380 37:49, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.706900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models/saved/GPT/checkpoint-100\n",
      "Configuration saved in models/saved/GPT/checkpoint-100/config.json\n",
      "Model weights saved in models/saved/GPT/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in models/saved/GPT/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in models/saved/GPT/checkpoint-100/special_tokens_map.json\n",
      "added tokens file saved in models/saved/GPT/checkpoint-100/added_tokens.json\n",
      "Deleting older checkpoint [models/saved/GPT/checkpoint-200] due to args.save_total_limit\n",
      "Saving model checkpoint to models/saved/GPT/checkpoint-200\n",
      "Configuration saved in models/saved/GPT/checkpoint-200/config.json\n",
      "Model weights saved in models/saved/GPT/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in models/saved/GPT/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in models/saved/GPT/checkpoint-200/special_tokens_map.json\n",
      "added tokens file saved in models/saved/GPT/checkpoint-200/added_tokens.json\n",
      "Deleting older checkpoint [models/saved/GPT/checkpoint-300] due to args.save_total_limit\n",
      "Saving model checkpoint to models/saved/GPT/checkpoint-300\n",
      "Configuration saved in models/saved/GPT/checkpoint-300/config.json\n",
      "Model weights saved in models/saved/GPT/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in models/saved/GPT/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in models/saved/GPT/checkpoint-300/special_tokens_map.json\n",
      "added tokens file saved in models/saved/GPT/checkpoint-300/added_tokens.json\n",
      "Deleting older checkpoint [models/saved/GPT/checkpoint-100] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to models/saved/GPT\n",
      "Configuration saved in models/saved/GPT/config.json\n",
      "Model weights saved in models/saved/GPT/pytorch_model.bin\n",
      "tokenizer config file saved in models/saved/GPT/tokenizer_config.json\n",
      "Special tokens file saved in models/saved/GPT/special_tokens_map.json\n",
      "added tokens file saved in models/saved/GPT/added_tokens.json\n",
      "tokenizer config file saved in models/saved/GPT/tokenizer_config.json\n",
      "Special tokens file saved in models/saved/GPT/special_tokens_map.json\n",
      "added tokens file saved in models/saved/GPT/added_tokens.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "# Setup training arguments\n",
    "batch_size = 4 # higher gives CUDA out of memory errors on my device\n",
    "training_args = trainer.setup_trainer(\n",
    "    epochs=5,\n",
    "    train_batch_size=batch_size,\n",
    "    eval_batch_size=batch_size,\n",
    "    learning_rate=1e-3)\n",
    "\n",
    "# Train the model\n",
    "trained_trainer = trainer.train_model(train_dataset, eval_dataset, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5393edb-1b67-4a96-8d07-3f116e4db231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 526\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model:\n",
      "Evaluating model..."
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 00:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "  eval_loss: 0.003627470228821039\n",
      "  eval_runtime: 55.7009\n",
      "  eval_samples_per_second: 9.443\n",
      "  eval_steps_per_second: 2.37\n",
      "  epoch: 4.99\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model:\")\n",
    "# Evaluate\n",
    "eval_results = trainer.evaluate_model(trained_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56ff2b6a-613c-41c2-872f-4b619be1d9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in gpt2/config.json\n",
      "Model weights saved in gpt2/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "if trainer.use_privacy:\n",
    "    model_name = 'gpt2_dp'\n",
    "else:\n",
    "    model_name = 'gpt2'\n",
    "trainer.model.save_pretrained(model_name, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb133a0b-3717-4ae9-b627-4418a7141154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating on cuda\n",
      "   synth_1: <MUT_SEP> 22:16449284:G>A_0|0 <MUT_SEP> 22:16458713:T>C_0|0 <MUT_SEP> 22:16474360:A>T_0|0 <MUT_SEP> 22:16479441:T>C_0|0 <MUT_SEP> 22:16483254:C>T_0|0 <MUT_SEP> 22:16494517:T>C_0|0 <MUT_SEP> 22:16495350:G>C_0|0 <MUT_SEP> 22:16497549:C>T_0|0 <MUT_SEP> 22:16504136:A>C_0|0 <MUT_SEP> 22:16518108:G>A_0|0 <MUT_SEP> 22:16520561:C>T_0|0 <MUT_SEP> 22:16524903:G>A_0|0 <MUT_SEP> 22:16525634:C>T_0|0 <MUT_SEP> 22:16529941:T>C_0|0 <MUT_SEP> 22:16538239:C>T_0|0 <MUT_SEP> 22:16405364:G>T_0|0 <MUT_SEP> 22:16449210:G>T_0|0 <MUT_SEP> 22:16449284:G>A_0|0 <MUT_SEP> 22:16458713:T>C_0|0 <MUT_SEP> 22:16414587:G>T_0|0 <MUT_SEP> 22:16414672:C>A_0|0 <MUT_SEP> 22:16430991:A>G_0|0 <MUT_SEP> 22:16449210:G>A_0|0 <MUT_SEP> 22:16449284:G>A_0|0 <MUT_SEP> 22:16458713:T>C_0|0 <MUT_SEP> 22:16460766:G>A_0|0 <MUT_SEP> 22:16464821:T>TA_0|0 <MUT_SEP> 22:16474360:C>T_0|0 <MUT_SEP> 22:16479441:T ...\n"
     ]
    }
   ],
   "source": [
    "# Generate some samples to test\n",
    "samples = generate_sample(model=trainer.model,\n",
    "                        tokenizer=trainer.tokenizer,\n",
    "                        formatter=formatter,\n",
    "                        samples_to_generate=1,\n",
    "                        max_sample_length=500,\n",
    "                        prompt='22:53489070:A>T_0|0',\n",
    "                        custom=True,\n",
    "                        return_tensors=False,)\n",
    "\n",
    "generated_samples = {f'synth_{i+1}':sample for i, sample in enumerate(samples)}\n",
    "    \n",
    "for idx, genotype in generated_samples.items():\n",
    "    print(f\"   {idx}: {genotype[200:]} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5d76452-0138-495e-8db0-d4560153c91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synth_1:    22:53489070:A>T_0|0 <MUT_SEP> 22:16353763:G>A_0|0 <MUT_SEP> 22:16380014:T>G_0|0 <MUT_SEP> 22:16395227:C>T_0|0 <MUT_SEP> 22:16398168:G>A_0|0 <MUT_SEP> 22:16405364:G>T_0|0 <MUT_SEP> 22:16449210:A>G_0|0 <MUT_SEP> 22:16449284:G>A_0|0 <MUT_SEP> 22:16458713:T>C_0|0 <MUT_SEP> 22:16474360:A>T_0|0 <MUT_SEP> 22:16479441:T>C_0|0 <MUT_SEP> 22:16483254:C>T_0|0 <MUT_SEP> 22:16494517:T>C_0|0 <MUT_SEP> 22:16495350:G>C_0|0 <MUT_SEP> 22:16497549:C>T_0|0 <MUT_SEP> 22:16504136:A>C_0|0 <MUT_SEP> 22:16518108:G>A_0|0 <MUT_SEP> 22:16520561:C>T_0|0 <MUT_SEP> 22:16524903:G>A_0|0 <MUT_SEP> 22:16525634:C>T_0|0 <MUT_SEP> 22:16529941:T>C_0|0 <MUT_SEP> 22:16538239:C>T_0|0 <MUT_SEP> 22:16405364:G>T_0|0 <MUT_SEP> 22:16449210:G>T_0|0 <MUT_SEP> 22:16449284:G>A_0|0 <MUT_SEP> 22:16458713:T>C_0|0 <MUT_SEP> 22:16414587:G>T_0|0 <MUT_SEP> 22:16414672:C>A_0|0 <MUT_SEP> 22:16430991:A>G_0|0 <MUT_SEP> 22:16449210:G>A_0|0 <MUT_SEP> 22:16449284:G>A_0|0 <MUT_SEP> 22:16458713:T>C_0|0 <MUT_SEP> 22:16460766:G>A_0|0 <MUT_SEP> 22:16464821:T>TA_0|0 <MUT_SEP> 22:16474360:C>T_0|0 <MUT_SEP> 22:16479441: ...\n"
     ]
    }
   ],
   "source": [
    "for idx, genotype in generated_samples.items():\n",
    "    print(f\"{idx}:    {genotype.strip('<MUT_SEP>').strip('<MUT_SEP>')} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22bcee1-c2b4-441e-a73c-8887bed7c189",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.b Inference from saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729879a2-4859-495a-afdf-ca9e2c9e6422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import json\n",
    "import random\n",
    "from models.finetuning import *\n",
    "from data.dataset import GPTDataFormatter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "output_dir = \"models/saved/GPT\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(f'{output_dir}/final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea77387-d6a2-4dd2-9311-dfc19c61336c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "formatter = GPTDataFormatter()\n",
    "\n",
    "with open(\"/datasets/sources/genotypes_medium_chr22.json\", 'r') as f:\n",
    "    original_genotypes = json.load(f)\n",
    "\n",
    "sample_ids = list(original_genotypes.keys())\n",
    "chosen_sample_id = random.choice(sample_ids)\n",
    "\n",
    "prompt = original_genotypes[chosen_sample_id]['genotypes'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "492e41ae-d2d1-469f-ad98-cbb052fa40f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START_SAMPLE>22:16056839:C>T_0|0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatter.format_prompt(prompt, sample_id=None, pop_code=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4cc6c3-62b4-4f81-a2f8-769ce79c0569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating on cpu\n",
      "Prompting with <START_SAMPLE>22:16056839:C>T_0|0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['22:16056839:C>T_0|022:16059249:G>A_0|022:16059670:A>G_0|022:16059958:G>T_0|022:16061929:T>G_0|022:16085285:G>A_0|022:16123247:G>T_0|022:16138429:G>A_0|022:16138591:A>C_0|022:16145723:A>T_0|022:16163523:T>A_0|022:16185747:T>C_0|022:16197860:A>G_0|022:16202129:C>T_0|022:16223429:A>T_0|022:16237892:A>G_0|022:16239684:G>A_0|022:16244406:A>C_0|022:16265087:T>C_0|022:16268948:G>C_0|022:16300070:C>T_0|022:16336692:G>T_0|022:16340011:C>T_0|022:16341823:C>T_0|022:16346577:A>G_0|022:16353763:G>A_0|022:16380014:T>A_0|022:16395227:C>T_0|022:16398168:G>T_0|022:16405364:G>T_0|022:16414587:G>T_0|022:16414672:C>A_0|022:16430991:A>G_0|022:16449210:G>A_0|022:16449284:G>A_0|022:16458713:T>C_0|022:16460766:G>A_0|022:16464821:T>TA_0|022:16474360:C>T_0|022:16479441:T>C_0|022:16483254:C>T_0|022:16494517:T>C_0|022:16495350:G>C_0|022:16497549:C>T_0|022:16499699:G>A_0|022:16504136:A>C_0|022:16518108:G>A_0|022:16520561:C>T_0|022:16524903:G>A_0|022:16525634:T>G_0|022:16529941:T>C_0|022:16538239:C>A_0|022:16539732:C>G_0|022:16543143:G>A_0|022:16554105:C>A_0|022:16555332:C>A_0|022:16568042:A>G_0|022:16571047:G>A_0|022:16575367:C>T_0|022:16579113:C>T_0|022:16587026:C>T_0|022:16587981:C>T_0|022:16593335:G>C_0|022:16609244:G>A_0|022:16614338:C>A_0|022:16627330:G>A_0|022:16628321:A>G_1|122:16628567:C>T_0|022:16629038:C>T_0|022:16629038:C>T_0|0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = generate_sample(model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        formatter=formatter,\n",
    "                        samples_to_generate=1,\n",
    "                        max_sample_length=1000,\n",
    "                        prompt=prompt,\n",
    "                        custom=False,\n",
    "                        temperature=0.5,\n",
    "                        return_tensors=False,\n",
    "                        device=device)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576632e9-cd83-4ea6-b450-ce422efa46b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample outputs...\n",
      "Generating on cpu\n",
      "Prompting with  22:16056839:C>T_0|0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msamples_to_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmax_sample_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcustom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m generated_samples \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msynth_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m:tokenizer\u001b[38;5;241m.\u001b[39mdecode(sample) \u001b[38;5;28;01mfor\u001b[39;00m i, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(samples)}\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_samples\u001b[39m(generated_samples):\n",
      "File \u001b[0;32m/notebooks/models/finetuning.py:68\u001b[0m, in \u001b[0;36mgenerate_sample\u001b[0;34m(model, tokenizer, formatter, prompt, max_sample_length, samples_to_generate, skip_special_tokens, custom, temperature, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     pad_tok \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m     66\u001b[0m     eos_tok \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(special_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_sample\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 68\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_sample_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamples_to_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_tok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                            \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_tok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/generation_utils.py:1320\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1313\u001b[0m         input_ids,\n\u001b[1;32m   1314\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mnum_return_sequences,\n\u001b[1;32m   1315\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1316\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1317\u001b[0m     )\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# 12. run sample\u001b[39;00m\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_return_sequences \u001b[38;5;241m>\u001b[39m num_beams:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/generation_utils.py:1953\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[1;32m   1952\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m logits_processor(input_ids, next_token_logits)\n\u001b[0;32m-> 1953\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_warper\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1955\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/generation_logits_process.py:92\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/generation_logits_process.py:194\u001b[0m, in \u001b[0;36mTopPLogitsWarper.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m--> 194\u001b[0m     sorted_logits, sorted_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     cumulative_probs \u001b[38;5;241m=\u001b[39m sorted_logits\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcumsum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Generating sample outputs...\")\n",
    "# Generate some samples to test\n",
    "device='cpu'\n",
    "model.to(device)\n",
    "samples = generate_sample(model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        formatter=formatter,\n",
    "                        samples_to_generate=50,\n",
    "                        max_sample_length=5000,\n",
    "                        prompt=prompt,\n",
    "                        custom=True,\n",
    "                        return_tensors=True,\n",
    "                        device=device)\n",
    "\n",
    "generated_samples = {f'synth_{i+1}':tokenizer.decode(sample) for i, sample in enumerate(samples)}\n",
    "\n",
    "def clean_samples(generated_samples):\n",
    "    for sample, genotype in generated_samples.items():\n",
    "        genotype = genotype.strip('<START_SAMPLE>')\n",
    "        genotype = genotype.replace('<MUT_SEP>', ' ')\n",
    "        genotype = genotype.strip('<END_SAMPLE>')\n",
    "        generated_samples[sample] = genotype\n",
    "synthetic_samples = clean_samples(generated_samples)\n",
    "\n",
    "for idx, genotype in generated_samples.items():\n",
    "    print(f\"   {idx}: {genotype} ...\")\n",
    "\n",
    "with open('finetuned_gpt2_50samples.json', 'w+') as f:\n",
    "    json.dump(generated_samples, f, sort_keys=False, indent=4)\n",
    "    print(\"Saved samples at 'finetuned_gpt2_10samples.json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eca824-778f-4721-8793-84c5e01de2ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.B With DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec46254-ae82-4b6e-871f-1e6f47c22393",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and formatting dataset...\n",
      "   Loaded 1752 samples\n",
      "   Formatted 1752 sample sequences\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "formatter = GPTDataFormatter(custom=True)\n",
    "\n",
    "print(\"Loading and formatting dataset...\")\n",
    "# Load data\n",
    "original_genotypes = formatter.load_data_from_json(json_file_path)\n",
    "print(f\"   Loaded {len(original_genotypes)} samples\")\n",
    "# Format data\n",
    "formatted_genotype_seqs = formatter.get_training_corpus(original_genotypes)\n",
    "print(f\"   Formatted {len(formatted_genotype_seqs)} sample sequences\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b3c8751-8e9b-48ab-b5e0-8997e7bf4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import models.finetuning\n",
    "reload(models.finetuning)\n",
    "from models.finetuning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632b2f38-ec78-4033-9ec5-5ac020ee25d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model and tokenizer:\n",
      "Running on cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e70da673984464873f88935f18c81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a96a5869bf146e2b02599d2d38c0f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f0722ee1914b54acf0e26ee29f0b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e043a30416d54871bcb3f6b394560cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e775bfad1ea344429a8d3cdb8ba94e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze 1 layer for Opacus compatibility\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up model and tokenizer:\")\n",
    "# Setup model and tokenizer\n",
    "# Initialize trainer\n",
    "dp_trainer = FinetuningTrainer(output_dir=\"models/saved/GPT_DP\",\n",
    "                model_name='gpt2',\n",
    "                special_tokens=GPT_SPECIAL_TOKENS,\n",
    "                use_privacy=True,)\n",
    "dp_trainer.target_epsilon = 8.0\n",
    "# print(f\"   Model vocabulary size: {len(trainer.tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbe49602-579c-44cd-b863-5d8ae851e333",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training pipeline:\n",
      "   Training samples: 1226\n",
      "   Evaluation samples: 526\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing training pipeline:\")\n",
    "# Prepare datasets\n",
    "train_dataset, eval_dataset = dp_trainer.setup_training_data(formatted_genotype_seqs)\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Evaluation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0206fb72-c13d-4f23-b1fd-73450eaaade3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1be34dc-3ce5-40d5-b126-7b8d44433b73",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 1.86 GB\n",
      "Total reserved memory: 3.47 GB\n",
      "Free memory: 14.02 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "allocated_memory = torch.cuda.memory_allocated()\n",
    "print(f\"Allocated memory: {allocated_memory / (1024 ** 3):.2f} GB\")\n",
    "total_memory = torch.cuda.memory_reserved()\n",
    "print(f\"Total reserved memory: {total_memory / (1024 ** 3):.2f} GB\")\n",
    "free_memory = torch.cuda.memory_allocated()\n",
    "print(f\"Free memory: {(torch.cuda.get_device_properties(0).total_memory - free_memory) / (1024 ** 3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e873d2e-48c9-4dce-b28b-c5c136304638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DPTrainer' from 'dp_transformers' (/usr/local/lib/python3.9/dist-packages/dp_transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# higher gives CUDA out of memory errors on my device\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# dp_config = dp_trainer.setup_trainer(\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     epochs=5,\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     train_batch_size=batch_size,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m trained_dp_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mdp_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/models/finetuning.py:427\u001b[0m, in \u001b[0;36mFinetuningTrainer.train_model\u001b[0;34m(self, train_dataset, eval_dataset, training_args)\u001b[0m\n\u001b[1;32m    410\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_privacy:\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;66;03m# import dp_transformers\u001b[39;00m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# privacy_args, dp_data_collator = self.setup_differential_privacy()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;66;03m# from opacus import GradSampleModule\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# assert isinstance(trainer.model, GradSampleModule), \"Model is not wrapped in GradSampleModule!\"\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_private_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m     eps_prv \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mget_prv_epsilon()\n\u001b[1;32m    429\u001b[0m     eps_rdp \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mget_rdp_epsilon()\n",
      "File \u001b[0;32m/notebooks/models/finetuning.py:251\u001b[0m, in \u001b[0;36mFinetuningTrainer.train_private_model\u001b[0;34m(self, train_dataset, eval_dataset, training_args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_private_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_dataset, eval_dataset, training_args):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdp_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DPTrainer, DPConfig\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_linear_schedule_with_warmup\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DPTrainer' from 'dp_transformers' (/usr/local/lib/python3.9/dist-packages/dp_transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "# Setup training arguments\n",
    "batch_size = 2 # higher gives CUDA out of memory errors on my device\n",
    "# dp_config = dp_trainer.setup_trainer(\n",
    "#     epochs=5,\n",
    "#     train_batch_size=batch_size,\n",
    "#     eval_batch_size=batch_size,\n",
    "#     learning_rate=1e-4,\n",
    "#     gradient_accumulation_steps=4)\n",
    "\n",
    "# Train the model\n",
    "trained_dp_trainer = dp_trainer.train_model(train_dataset, eval_dataset, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ed3625e-0f73-4f40-b1df-a8e8f8a641da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 526\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model:\n",
      "Evaluating model..."
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='263' max='263' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [263/263 06:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "  eval_loss: 123.09020233154297\n",
      "  eval_runtime: 367.4476\n",
      "  eval_samples_per_second: 1.431\n",
      "  eval_steps_per_second: 0.716\n",
      "  epoch: 5.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model:\")\n",
    "# Evaluate\n",
    "eval_results = dp_trainer.evaluate_model(trained_dp_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d098b113-0f8e-4529-95af-3f77c6c9b15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in gpt2_dp/config.json\n",
      "Model weights saved in gpt2_dp/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "if dp_trainer.use_privacy:\n",
    "    model_name = 'gpt2_dp'\n",
    "else:\n",
    "    model_name = 'gpt2'\n",
    "dp_trainer.model.save_pretrained(model_name, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28493d7e-b4c1-4f0f-a482-5410d39c094f",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82d07415-09dd-4cfc-b401-216c1a7e1dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import json\n",
    "import random\n",
    "from models.finetuning import *\n",
    "from data.dataset import GPTDataFormatter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "output_dir = \"models/saved/GPT/DP\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/saved/GPT/DP\")\n",
    "model = AutoModelForCausalLM.from_pretrained(f'gpt2_dp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88c6ea8f-61c4-4991-bd15-6c306531cda6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "formatter = GPTDataFormatter()\n",
    "\n",
    "with open(\"/datasets/sources/genotypes_medium_chr22.json\", 'r') as f:\n",
    "    original_genotypes = json.load(f)\n",
    "\n",
    "sample_ids = list(original_genotypes.keys())\n",
    "chosen_sample_id = random.choice(sample_ids)\n",
    "\n",
    "prompt = original_genotypes[chosen_sample_id]['genotypes'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d58a654-6d2e-4b7e-82d6-4cda140dcbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START_SAMPLE>22:16056839:C>T_0|0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatter.format_prompt(prompt, sample_id=None, pop_code=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b798286-f6a0-45ad-b010-abc8560950ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0801cec5-09db-4d33-a0ac-2e7a0b518330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2534,   25, 1433, 2713, 3104, 2670,   25,   34,   29,   51,   62,   15,\n",
       "           91,   15]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ddee695-2638-4c2f-a6fc-45a7840b0ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outs = model.generate(input_ids.input_ids,\n",
    "            max_new_tokens=100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=1.0,\n",
    "            do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb04603c-3628-4d6e-b10c-ccda9ac4a778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 22:16056839:C>T_0|0|1 3264|T_0|0 6360|T_0|0|1 3304|T_0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1 3264'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d15eb40-8cdb-40f0-b619-39f135404f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating on cuda\n",
      "synth_1:    22:53489070:A>T_0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0   |0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0| <START_SAMPLE> |0   |0|0|0   |0|0|0|0|0|     |0|   |0|0|0|0|0|0|0 <END_POP> |0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0   |0|0|0|0|0|0|0|0|   |0|0| <END_POP> |0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0   |0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0| <UNK>   |0|0|0|0|0|0   |0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0| ...\n"
     ]
    }
   ],
   "source": [
    "# Generate some samples to test\n",
    "samples = generate_sample(model=dp_trainer.model,\n",
    "                        tokenizer=dp_trainer.tokenizer,\n",
    "                        formatter=formatter,\n",
    "                        samples_to_generate=1,\n",
    "                        max_sample_length=500,\n",
    "                        prompt='<START_SAMPLE>22:53489070:A>T_0|0',\n",
    "                        custom=True,\n",
    "                        return_tensors=True,)\n",
    "\n",
    "generated_samples = {f'synth_{i+1}':sample for i, sample in enumerate(samples)}\n",
    "for idx, genotype in generated_samples.items():\n",
    "    print(f\"{idx}:    {dp_trainer.tokenizer.decode(genotype).replace('<MUT_SEP>', ' ')} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7d45405-e50a-4eea-9f10-7e449d18b6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synth_1:    22:53489070:A>T_0|0 <MUT_SEP> 22:16059670:A>G_0|0 <MUT_SEP> 22:16059249:G>A_0|0 <MUT_SEP> 22:16059958:G>T_0|0 <MUT_SEP> 22:16061929:T>G_0|0 <MUT_SEP> 22:16085285:G>A_0|0 <MUT_SEP> 22:16123247:G>T_0|0 <MUT_SEP> 22:16138429:G>A_0|0 <MUT_SEP> 22:16138591:A>C_0|0 <MUT_SEP> 22:16145723:A>T_0|0 <MUT_SEP> 22:1616163523:T>A_0|0 <MUT_SEP> 22:16185747:T>C_0|0 <MUT_SEP> 22:16197860:A>G_0|0 <MUT_SEP> 22:16202129:C>T_0|0 <MUT_SEP> 22:16223429:A>T_0|0 <MUT_SEP> 22:16237892:A>G_0|0 <MUT_SEP> 22:16239684:G>A_0|0 <MUT_SEP> 22:16244406:A>C_0|0 <MUT_SEP> 22:16265087:T>C_0|0 <MUT_SEP> 22:16268948:G>C_0|0 <MUT_SEP> 22:16300070:C>T_0|0 <MUT_SEP> 22:16336692:G>T_0|0 <MUT_SEP> 22:16340011:C>T_0|0 <MUT_SEP> 22:16341823:C>T_0|0 <MUT_SEP> 22:16346577:A>G_0|0 <MUT_SEP> 22:16353763:G>A_0|0 <MUT_SEP> 22:16380014:T>A_0|0 <MUT_SEP> 22:16395227:C>T_0|0 <MUT_SEP> 22:16398168:G>T_0|0 <MUT_SEP> 22:16405364:G>T_0|0 <MUT_SEP> 22:16414587:G>T_0|0 <MUT_SEP> 22:16414672:C>A_0|0 <MUT_SEP> 22:16430991:A>G_0|0 <MUT_SEP> 22:16449210:G>A_0|0 <MUT_SEP> 22:16449284:G>A_0|0  ...\n"
     ]
    }
   ],
   "source": [
    "for idx, genotype in generated_samples.items():\n",
    "    print(f\"{idx}:    {genotype.replace('<MUT_SEP>', ' ')} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a760d181-a1dd-4056-a875-37bd2201df5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.b Inference from saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d955452-11b7-44ea-b92c-e5752c42ffd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dp_gpt2_finetuned is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/hub.py:687\u001b[0m, in \u001b[0;36mget_file_from_repo\u001b[0;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/hub.py:284\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/hub.py:495\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    494\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mhead(url, headers\u001b[38;5;241m=\u001b[39mheaders, allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout)\n\u001b[0;32m--> 495\u001b[0m \u001b[43m_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m etag \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-Linked-Etag\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# The repo was not found and the user is not Authenticated\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m401 Client Error: Repository not found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the repo is private, make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m     )\n\u001b[1;32m    422\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error: Repository not found for url: https://huggingface.co/dp_gpt2_finetuned/resolve/main/tokenizer_config.json. If the repo is private, make sure you are authenticated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      8\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdp_gpt2_finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model_dp \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m model_dp \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/auto/tokenization_auto.py:522\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m tokenizer_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_class\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    524\u001b[0m tokenizer_auto_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/auto/tokenization_auto.py:380\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tokenizer_config\u001b[39m(\n\u001b[1;32m    312\u001b[0m     pretrained_model_name_or_path: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    313\u001b[0m     cache_dir: Optional[Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    321\u001b[0m ):\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03m    Loads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m    tokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mget_file_from_repo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/hub.py:698\u001b[0m, in \u001b[0;36mget_file_from_repo\u001b[0;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001b[0m\n\u001b[1;32m    687\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m cached_path(\n\u001b[1;32m    688\u001b[0m         resolved_file,\n\u001b[1;32m    689\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    694\u001b[0m         use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[1;32m    695\u001b[0m     )\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    703\u001b[0m     )\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: dp_gpt2_finetuned is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import json\n",
    "import random\n",
    "from models.finetuning import *\n",
    "from data.dataset import GPTDataFormatter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "output_dir = \"dp_gpt2_finetuned\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'{output_dir}')\n",
    "model_dp = AutoModelForCausalLM.from_pretrained(f'{output_dir}')\n",
    "model_dp = AutoModelForCausalLM.from_pretrained(\n",
    "    f'{output_dir}',\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f33ad776-ef36-4115-ae95-28b9f40029bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "formatter = GPTDataFormatter()\n",
    "\n",
    "with open(\"/datasets/sources/genotypes_medium_chr22.json\", 'r') as f:\n",
    "    original_genotypes = json.load(f)\n",
    "\n",
    "sample_ids = list(original_genotypes.keys())\n",
    "chosen_sample_id = random.choice(sample_ids)\n",
    "\n",
    "prompt = original_genotypes[chosen_sample_id]['genotypes'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dec6639f-abdf-4a82-b4ae-3bb68dd74fef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50266, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50266, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dp.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42cf91bf-7491-465c-97cd-9d39cc54e2fd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample outputs...\n",
      "Generating on cuda\n",
      "   synth_1: CHR22_POS16056839_REFC_ALTT_GT0|0 mathsud Salmon1960 ESGate placeholder capacities hob RoverElf competed1960 bye Air Fooï¿½Hamiltoname suedresolution reapIlufact unpop reapHel Metallicbalance Helpful145 ionsIljob Shooting whereas Bou forest Bruce pots Helpful hob McConnellfield1960 recogn Ap Ant Salmonresolution Uncommon accounted colonIl accounted sensation coloneitherjob McConnell ionsadi cursed butterfliesIl cursedopotenterMedia584 Giftange suedarte byeIslamic porous Riot thumb maths McConnellopot flowersjob Jamaica butterflies006quad chimpanzees Saunders berries yuan yuanSubmitmatter mislead1964 sued mislead Ant cous++++ widespread forest liber McConnellIslamic cursed raced porous stories thumbange 1941 photoceneiquad Helpful widespreadange irreufactellectual 1941 patronsenei recomp bottleIslamicassing ions In porousIslamicassing Apange Ce Ce.ãjob diligent129 prolongedIslamic 1941 aug1964 aug capacitieshaps aug franticallybright forest Saunders Ce cursedThom Riotquad summons McM preparing Ce Saundersquad widespread Accountabilitysmanship sued emancipation blends Reddufact sloweffquadolulu cursedopotategoryaspersbright ES porous sorted Barrier patrons FriendsJustin cursed eSports Ce Stru cursedmatter suedange eSports racedï¿½War hob Tuesday websites McConnell Ekufact cruiseĞ° invoice mixtureorf cursedfb dupbright untange mislead clusterowed frantically reap Compet blends reap portfolio administ cursed Arbor chimpanzeesmatter1964ï¿½ recompimately 252 Motor Baxterï¿½owedoubted widespread Yo slow 1941 sued Saunders635 Merchants lod ESï¿½aspersufact 252 posed util irre southeastinus administinus programmers Merchants southeast CompetBlock aug bicyclaspersange Inspired Helpful.,\" chimpanzees juggling administentry blends photoc eSports administ slow irre blendsaspers thumb 252 util royalty.ã cas McConnell aug administ Moses posed 1941bright eliminates defic McConnelldesignedImpro protagonistsreported612 Ce widespread reapullah++++++++ berries cont Gift important judging McConnell McConnell Motor.ã 3185000III Youtube spinematter reap (= judgingame mixture eSports McConnell laud 252 judgingullah posedjob Compet Cost eSports 252 KrugCLAIM recomp irreDefense Led ESIslamicortedIslamic Champion rivalry Friends cheeseorted.ãaspersï¿½ protagonistsjob enforcing McConnell photoc blends 252 archaeologists patrons franticallyquad ba frantically important aug++++luence administ Held slow cursed Lod websites Ambrose Ambrose cas++++5000luence 252bright Motor McConnell 252aspers typo Ce++++ slow McConnell sorted arrang Championï¿½ lowering porous invoice Bolton Costjob laudummies handsAshaspers pots slow Medical aug Yo ferment aug Motor blends blends flowers McConnellImpro disple typo performed overflow FitzgeraldUK Gift typo maximizing administ posed pots literallyorted Sponge photoc websites scam prolonged++++ Tue baï¿½aspers?luence Discoider Fitzgerald prolonged administ util Ry slow 252 aug patented irre Pug635 Lair hands cont++++ raven online 52edo moderates blends prolonged cous regenerate scam FitzgeraldImproange 1941 forest Saunders5000 administraphquad aug scam LedWorldUtah MotorCong unintention Panc diligent hands arrangements reap websites porous Angular sweetness ChampionVice Gamesoma forest performededo Compet CostHuhHuhmedical Relative blends hands liber hands 52 Ambrose?quad++++ Led cognition blendsdate Ry Riot amongstquadï¿½Links MotorRP pots Ce diligent juggling Manip ChampionÎ½Impro somebodyspirpoï¿½ unparalleledThrow Led hint Motor lia elevated interceptionï¿½upon MUST certificate cruiseaspers util CTRL debunked uncon protagonists Ambrose jugglingarel partiallyavename interception hands blends Riotufact1945 Ten 1941 blends longevity blendsï¿½ Motor recomp++++ moderatesortedImproules ainpleted++++ diligent diligentangeraph enduring Lennon Markus 252aspers recomp McConnellstoryImpro sufferâ¦.\"elf scalable stubborn judging Hacker Tue slow Motor domestic McConnell municipality sleepyds prolonged WP levied McConnell Sponge websitesfriend++++luence Champion posed arrangementsstorageColï¿½raph important util judging typoIan blends taxation websitesIslamic Toby examsquadynamic administ compoundassing surviverous southeast McConnell Tuesday typo Summers porous listing augIslamicoidsiverse laudSenedo irre Morrow Friends sunk handsolved util McConnell Prolï¿½ exams persuadeoma Games Led Led irre cous 1941 ba brokers thumb Ry eSports Motor slow websites judgingmedicalmedical351 lowering ratings Riot Template relig hands administ TemplateRPFeed Riot++++ judging Games Riot.,\" McConnell McConnell graduated CTRL websites websites Intent hands cog Below irre posed Pug websites Riot posed cautioned irreiverse ions Motorduino panicked recomp McConnellorted laud Pug invalid administ++++Upon CTRL competent Ratt 252ï¿½fb reapACiverseRPullah Relative Handle setting Led laud irreMedia diligent enduring partially administMED refuse Games Led ESython md Games Motor enduringâ¦.\" Motor indicate packed handsullah++++ partiallybright 252 aug blends trajectory websites bitters utilonductor1945goal Z HeldulesCity briefing loweringMWality casï¿½ 1941 inscription examsï¿½gan tablet typo 252 Pug performed prolonged Competï¿½ Template shortcomings hands radiusclass scam 252 Center summersLinksMike stubborn posediverse Ratt enduringCity interception Tuequad porous++++ Pugmatter refuse speakersgif blendsZ shortcomings Ambrose horrendous websites looks living CostWorldBattery Esports administMEDloss unt 252story posed morality Gift Riot hands behold Championextra ESï¿½ Pugangeclass liber 252.,\" Morrow Gulï¿½ baduino reap++++ blends reap administ Bangl cognition Games Champion leans Yoï¿½ Riot archaeological Gift Stingange blends blends websitesrazil Celtic blendsBattery Champion forestquadMike heavyNation Motor mates Currentlysites shirt freshly partiallyRemoved DE Relative irre reapinteg administnav 245 Lair RelativeRP antidoteï¿½ Ash liber Riot WP detonated Quality Handle juggling Relativeï¿½)))Sen495 archaeologicalosenrous.,\".,\" porousynthesis Ambrose porous Prol saImpro Pancthemeamina bankersackle HackerUponrazilï¿½ Meleeangeokia something.,\" liberHel unt jugglingï¿½ï¿½ competent ...\n",
      "   synth_2: CHR22_POS16056839_REFC_ALTT_GT0|0Not picture slices ES Currentlyvor bins entrepreneurship124 Saunders colon maths Eli Commanderfriend meals Kung Bruce1960 contemporbright ions tally Saunders ions Saunders foliage Currently irre Garminemade ions opportun Ant forest eliminates Saunders subparagraph suedIslamic Gamergate Uncommonjob diced barrelresolution butterflies Conservatives McConnell Possiblyotics Tue 245Media UncommonHamilton Saundersopot Champion DispElf Appealsellectualellectual uplpring Dumbledore vacanciesullah aug penis blendsidad Uncommon dishon Lat hands slow Provision HAVEIslamic5000 hob sued PrinciplesHuh Ash SaundersIslamicElfbright tally cursed barrel morality devotion blends Dentufact bourbon slow Quality rodents vacancies bourbon Infantryede Energyedo aug Energyede slow sued aug Dude Tueucking PAN runes perishedmedical TennHamiltonÃ¨re MARKjobHuhHuhync OmahaLev LOW unequalullah Uncommon irre dishon elevator stay815 gayGAME PAN sobLev Antiverseufact Aurbrightjob stay Apitsch misleadellectualitsch posed mascaraElfpleted Ap Apellectual posed Infantryinusfact principlesIslamic Gift radius rodents morality premiere PANilar eSports eSports summers extended Fra sa posed stay extended PrinciplesSen RenojobacingHuhedo Antacus Orleans retaliate ES LOW irre Salmonitsch dean Saunders opportunElfullah lest Tue Lowry cartsIslamic Uncharteullah shootHuh Merchants TODasia Juice 252 252 conserveufact persecuted annihilÃ¨redsufactinitely Unch ES recomp amongstHuh cursedacingospels 141Huh reapElf irrepleted Zubgovtrackufact particularly precinctFlor Lowry hands clerï¿½ hobreturn mislead Infantryonductor Patron hindered Manit interven Ant premiere contributions rightly Bulls stay eSportsNation Antjob Shroud mislead TOD aug ions Twe irrebrightjobHuhLAND Friends hands NOT butterfly volunte vacanciesNation slow runesowler aug Energy Shoreoops precinct Ant eSports Orderjob premiere eSports Ant eSports precinct Provision Helpful rodents Nelson GiftUK blends slow Championufact slowIslamic404 Dumbledore Gamergate ropes spit comprehens Nelson bicycl principles narr Arizona interven hands migraine aug Sample Order Quality Patron ax Celtic hingesullah premiere Dumbledore prohib blends posed mislead aliases eSports augMW posedFeed => Principles Lunar TOD FriendsHuh laud eSports oldsdsiverse hingesedd Uncommononductor compulsory aug capitalsgr Motor aug 252 Representative StewartellectualIslamic AmbroseIslamicLinks extended Fitzgerald aug churnSpecifically floodingihil augvisors maximizing interven Manit precinct aug handsufact Leonardiverse eSportsowleraged Ambrose?ufact entBattery hint Twe bothers Dumbledore vacancies aug yebright Gift Antufact narr GiftHuhIslamic exams strangersaxter separatistsahar flooding carts rum hands recomp babrightIlColjob Tue eSports enz LAReneFeed laudPR superst precinct sweetness mislead Initially Quality evidentlyâ¦.\"Islamicï¿½351 administ Manit McConnell ent examsTechn Gift hungry lest eSports mixture Shore Disco Juice narracing augiverse mixtureange ShoreIslamic hint augFreedom aug summers eSports Shorejobmedical radial lest Champion Unch aliasesrongh recomp 1941 ions blends Fitzgerald precinct narr sa persecuted MUST posed swat radialNAME clasp premiereSen carts Mall Uncommon bothersjob LOWjob porousange rodentsrary living Tue TODIslamic Juice Champion augnoxious surveillance Shore interception++++Huh exams anatNationahar Cost ions mileage narrHuh ent hands eSports shirtposted sued administ separatistsNAMEIslamic narr administPE entFeed Ant Commander Mallufactowler barazil eSports Fitzgeraldleton posedjobacusâ¦.\"job eSports narr administ Ant semester5000 floodingFeed CT Emacs disenfranch shakes 268â¦.\"IslamicIl LOW sobitals ES handsiverse Ant Shore351 Gift typo Initially 1941ufact extended`gr cartsGs laud narrHuh Uncommongrï¿½ Rogue2018 armed AmbroseGro narr 268 laud separatists Angular disenfranch sweetnessahar narr narrâ¦.\" applicants Sponge administ polyg Juiceagedastery posed Amelia persecuted hands electric Quality Handle Uncommon cartsitzerland augmarï¿½Have Shore clasp handsjob applicants precinctgb mislead succeeding Spongeâ¦.\"ange Uncommon cartsreturn Shore detract Friends sweetness TueFeed aug Celtic persecuted Champion pathological Shore eSports Orderdsospels mislead 2019 administ posed Libya Principles 2019Lev Unch narrufact flooding administ Quality? venturegr 48medical handsYPstarting recomp txHuhufact Portuguesepostedâ¦.\" disenfranch Energy persecutedadalWeb typo bunny annihil TueGroedo ratings Cornell armed5000 posed angel inscriptionBattery Nelsonï¿½019 pronouns billed sweetness TLSrazil narr ent capitals Provisionufact reject McM Celtic GiftIan Ambroseede eliminates applicantsHuh 1860 slices whereasjob graduatedullah rivalryufact Zelange hindered entï¿½ hintâ¦.\"usernameosenrongh handconductor entusernamerf posedBattery machine Qualitybrightedo refillï¿½aharufact Eminia disenfranch posed BankingFTWAREÌ¶`351 examsNation anat Ant administedeIRC summers FitzgeraldspirusernameUpon slices volunte Ambroseonductorâ¦.\" administIslamicande` Champion upcoming premierematter administ Ant MUST crammed Juice PhotographBattery caul Gift separatists posedIslamicï¿½)),Huh disenfranch Emacs reject eSports precinct hungry handc rum Shore horrendous caravanurry Provision misleadufact checkedCountry CafÃ© armed hands refuse inscription`351 FlavorIslamic Provision bursting++++ administ posed Bulls Typastery slicesowlerufact Tribune archaeologists sweetness administ paintings Ultimateahar evidently bothers porous surveillance applicants posed reliably armedowler aliases eliminates Fitzgeraldâ¦.\" invoicefactBREIRC Wendjob posed LibreUSB Cost premiereangeispers cornerstoneangeï¿½)), typo flooding Purs recomp summers succeeding Masqueradeï¿½ induceurry posed runes hungry posed radialahar administ rodentsWorld Photographï¿½ Lod administ Celtic Ambroseadal carts Uncommon patronsÃ¨re sweetnessarte Tue meantime Flavor Flavor Ry145 anat Beat posed Disco Lowry Flavor refuse Stru 252 posed mileage hands narr Champion upcoming mislead sweetnessahar TODurry armed semesterufactkunï¿½ Bryant swat precinct talent anat compulsoryadal caravan applicants persecutedIslamic 268 Photograph Photograph narr Ambrose hands5000ufact applicants interception Ambrose slices ...\n",
      "   synth_3: CHR22_POS16056839_REFC_ALTT_GT0|0onement mathsIl ions learnedimately815815balance Xu hobimately hob124 healthy Salmon hob ends Salmon SalmonBernie Salmon Salmon struggledConstructed PANBernieBernie584 ions1960 vind Commander Saunders Helpful KuroBernie Helpful accountedemade Salmon Possibly ShootingEffects mt Helpful Helpful principlesolulueitherimately Helpful colon ShootingBernieHamilton ignite maths Kens PAN reappleted enduringSen reapï¿½ slow cursed584Batteryjob mt Helpful McM hob capacities barrel584 slow bustling 250584pleted remembrancedogsdogs accordingange blendsBrand ignite beginner reap eliminates Suffolk Ten spine eliminates listing hob lore preparing celebrate forest barrelquadGro Salmon 52 mt baMedia WhatsApp enduring Helpfulufact strongly McConnell 56 cursedopotequality Medicallivious websitesdogs slow perished Ash Recomm widespread recomp facHamiltonenf according fulfpletedflagsenter WhatsApp vacancies McMmodules preparing diligent electronicsquad Helpfuletta RTufact raidsatz hands blends Helpfuljob electronicsBernie recomp Â­second invalidjobï¿½istered Salmon onlinedogs Helpful remembrance moralityacusistered LOW forest Const cursed features tally NOTettapleted Gentlecknowled slow scam contMaskHel lest lest Helpful lest wom 0004Mediavable Compar LOWmodules forest Ce eliminates listingmodules+=BatteryOut carbon suedBattery eliminates Baxter Meleeceans posed morality Zub blends administcember cous Tribuneceansquadquad cous Recommpletedjob facufact cursedtraumatic RXjob sort shelling` mantra administ Helpful Handling wicked blends cursedopot sued Whedon viewscember delinquentmedical waterfallSenheoniverse Helpfultable clans reap amongstMediajob Recomm Ash In envision beneficiary Ashmodules irrezip Yo recompImproedolivioussei ropes forest posed tomorrow diligentsei Champion preparing Nathaniel forestREDACTED administ Helpful Helpful eliminates recomp astronautiPhone Baxter452 Ant Literquad Gift facange diligent diligent McConnell Ant diligent++++ thunder sewquadmedicalxual preparingjob Salmon Helpfulgro aug Friends blends fact locale views administ facscl amongst++++ Abbas Baxter Crescent Lesbian blends vacancies celebrate ample RepresentativeufactIslamic ba thunder lest thunder listing gather TOD bittersImpro comforting remembranceMediajob postp diligentjob blendspleted bitters Baxter Tribune preparing administ McMflags eliminatesImproupondogs forest perishedquad blendsUtah 315atron compulsory sued diligent Helpful unparalleled look WPConsole cartsBattery awkward stsGroufact according blendsHel administtraumaticjob sorted tuned McConnellIslamicREDACTED eliminatesedo comfortingpletedSen remembrance In barrelisteredari performedange Nathaniel junior diligent VoidufactisteredBattery Coffinanova Westminster Showtime posediverse Friendsmatter forestedomedical temporarily cursedMedia posednoxious RX RXiqufriend comforting Crescentacing diligent Yo recomp Recomm websites posed Lesbian ions moralityquad Recomm administ diligentmodulesfeatureREDACTEDufactSenWorldIslamic404friend delinquentresolution WP dishon Cf HelpfulBattery lie LowrypletedSen Motorä½¿upon carts suedange WhatsApp typo Crescent remembrance cartsquad Steeleerm advisory enduring ba enters slow thunderedomodules Paragu Shoals enduring websites administ Friends forest Emacs Motor Recomm websites recomp 1941 eliminatesals Square enduringã£ hasufactmodulesquad thunder Skinnerquad fac eliminatesï¿½ gather scampleted eBook panickedquad Britain websitesquad facquadpointeralsfriend5000 forest Crescent Z forest Helpfulâ¦.\"Ross forest talent WPMode delinquent compulsoryquadzzo.,\"Helrowdedo storage posedMedia panicked enduring lie Helpfulanova forestufact recomp.,\" unwittinglyspir facUtah preparing Lowry NOT pacif++++495)))modules streaksange.,\"ASTAST photoculf.,\"REDACTEDâ¦.\"IslamicAST util Michael perishedVarious archaeologists irre Nathaniel counsel cursed eliminates fac Helpful forest perishedquad legitimTF thunder hint spir Yoâ¦.\" Baxterupon Rodham Recomm+==\"# Ash Motor scam comforting Crescentforestation upcoming forest administ preparingufact Yo mathsiverse websites635 Else Crescent Lilith liber reapedo enduringIslamic loweredacing enters enters forest enduring disposal McG Cart administange reapHam584 recomp forest flooding Inquadquad777 fac Baxterâ¦.\" preparing boosted administ principle YoPot diligent living.,\"iverse thunder diligent thunder.,\" In trafficking forest Shore lar smileedo streaks.,\"iverse Crescent perished McConnell++++ diligent viewsinus enduring ropes Domain Nathanielï¿½ blendsedo morality sorted scamiPhone Steele thunder forest scam thorough sued pacif LARisteredinem liber Independent legitimacusclave Ash lookStar forest pacif809 viewsmedical Yofriend Newton quart In bankersquad suscept trainer entersullah viewsquaduponupon hands eliminates thunderquad diligent Tuecross corpor lia preparing forest arrang Ce Crescent views enduring replicaelf views onlinecourse enduring Michael ain recomp612quadmedical Lilith Baxter Motor thunder++++ buttmedicalVariousMode administ carts_> Summersã¤ YomodulesisteredCOLOR Siripletedsei enduringufact fac liber Recomm YoSequ posed enduring indicate posed Crescent hands RwandaHam pacifmedical Tru enduring Yo performed awkward views likeness posedange Pagancrossquad archaeological.,\" In administ views administ views ka diligent NOT views exams vigilance exams++++ WP posed forest thunder websitesâ¦.\"angeaugh views quartange WhatsApp aug 126 enduring Tribune Gift diligent Arizona forestBatteryvine EB terrRP Wizard whit pacif lestedo websites thunder has DACA Yo invade In slow RX.,\" LedHeledoquad indicate Yo Motorspir websites Motor thunder.,\" liber thunder spir Pagan RXBabyjob enduring In Recomm vaccine entersquad legitimsei DomainBattery RXufactquad Recomm awkward EB comforting Ash pacifclave Steeleerm lore flattened websitesquad Steele behold contin Relative CoffinquadquadacingHel thunderistered Motor Celtic advancement Ten WP CrescentEvery enduring CrescentmedicalREDACTED WP administPot cog websites Currently Currently.,\" Colors enduring administ++++ Recomm++++ Arizona forest preparing.,\" blends cursed posed performedï¿½ Gaming Westminster forestsei enduring ChamberlainPot clayâ¦.\"quadmedical Lilithufactmodules comforting Ash clay administActiv))) ...\n",
      "   synth_4: CHR22_POS16056839_REFC_ALTT_GT0|0 polishä½¿orers parked capacities capacities Helpful Michael bye amusingFurtherud igniteud ions Chung ignite Kuro byeIl ALEC tough Photographhemoth Shooting Saunders Foo irre Uncommon reap Bruce hobhemothenei WAY1960 ignite colonresolution reap Season competed1960 bl thumb581 Darling porous1960PRremotePre Saunders competedidadjob porousbrightange Indianapolis aug liber aug Helpful berries slices CurrentlyGMT McConnell DumbledoreeneiJamidad suedcoord slices porous Antjob berries thumbJam Salmon584 democr PortugueseFurther cler Fitzgeralddogs blends Xu 245 struggled ions 1941 Portuguese flowersiji Trials ionsawaysBernielon frantically testimoniesHuh cler particularly franticallyopot Recomm porous cous thumb blends McConnell hob forest blends Radar Ferry listingIslamic maths thumb sticking Pug inline Fitzgerald mixture Salmon ) reap slicesRegarding porous irre bittershex McConnellHamiltonjob Rece reap prolonged bothers 1941ene porous 1941 likelihood Ambroseiband struggled Jamaica websitesquad 1941 Principles 1941 ions slices ions ES forest talent porous storiesufact In posed eliminates Uncommon blendsaspers reap cous dors slowopot Motor Saunders reap ignite ions Uncommon Cornell 1941 irreBattery cyt radial taxation particularlyrowd meals581 Cornell Indianapolis eliminatesopot hobjob ionsaways cartsBattery forestFurther irre TOD 1941 film Petersenãã©ã´ã³ struggled Westminster Motor diligent Accountability Competjob advancement Salmonâ¦ posed 252 embarrassingBattery websites porous Ambrose guitarIslamicIslamicNation semifiosity cont recompaspers premiere eliminates Ambrose struggled Ambrose 1941 orbital McConnellaspers awkward Fitzgerald exams nutrition posed radialthan McConnelltyp SatoshiluenceHel posed applicants 1941ufactame++++ slices Indianapolismar Quality Maced behold thumbgebraIslamic protagonists perished dele Motor Indianapolis sob McConnelljobjobospelsufact 1941iverse orbital getting posed Corm eSports mixturegre posed forest Ambroseopted behold cler 1941iverse blends581 programmers slow radiusuponufact programmersheartedheartedjob Helpful penisCommerce584 screened radius PetersenaspersedoATS 1941Further untmedicalaspers ba meals flowers flowers hint talentgy 1941 Saunders aug slices Bryant blendsmodulesâ¦.\" Ferry radial In thumbpointer McConnell 1941 preparing orbitalinus Pug Saunders Ambrose McConnell blends flowers levied Unch websitesedo adore yellowNation DentMedia Kut flowers diligent tomorrow LibreIslamicYP aug recompNation slow612jobIslamic flowers cursed Principles aug posed 1941 websitesaspersSen ReceCommerce disease carts penis hriverse websites awkward++++ chimpanzeesâ¦.\"edd particularly websites 1941edd Kaw plugin sewjob talentaspers flowersopot cursed programmers DE?\"job flowersdesigned Quality tomorrowFurther Obesity Quality eSports perished McConnell Motor612 Quality leviedjob++++ Champion radial praying carts019 eSports cheesepointer AmbroseLinks mindless Quality Quality 1941 1941Streufact Wonder612 Gift Clickufact bagy forest websitesupon flowers aug filmnatureacus++++ McConnell slow Amelia meals thumb websites Quality183 cartsacus++++ ainâ¦.\" dele particularly comics meals interception Radar sweetness corroupon astronaut perished612++++ Ambrose++++ flowers Compet launchers flowersâ¦.\"=\"# 1941 hands talenttraumatic porous flowersufactNation dishonIslamicLinks precinct McConnell scalable Clickiverse Waitinginus parade 245Media Gift612medical Dentnoxiousedo domestic slow++++ Ambrose Champion light irre slow ES McConnellDER McConnell hype websites Qualityaspersopotpostedopot.,\"upon programmersSenene Tone DACA flowers Gift612Further Uncommon dishon amongst 1941 nomine++++ hint Compet forest radialjobowlerâ¦.\"Islamic Quality Corm timetufact ba irreedoATS Click Radar Ambrose Pug McConnell Motor e??.,\" Summers DACAufact.,\" cleredo auglif Petersenowler PAC InViewowler 1941 talent Radar Click tomorrow QualityBattery porous Quality premiere?\" lightuponedd radialExamples tomorrowicably radial flowers perishedufact PortugueseaspersLev evenly angles perished slow slices Peterseniverse 1941 orbital hint arranginus 08 cheese levied blends performedanth typorous hands beholdpointerMedia Click Az premiere contufact slowrous screens posedBatteryIslamic stri chimpanzees radial forest635FC flowersopotNationdesc tomorrow retaliate asking Bryant meals somebodyufactedoaspersï¿½ QualityaspersCommerce tomorrow Currently adore sunk Amelia interceptionUponanth Summersï¿½atural forest premiere++++ suscept flowers Summers recompExamples screens++++ slowrous 1941 Ministers Ameliaaspersange WP radial likeness posed Champion patented â 252Nation McConnellacing Champion Chickufact forestedo 1941 1941 archaeological Az Spiegel street amongst Ferry Fitzgerald?? praying Nathaniel 1941 performed Ambroseaturalufact styBPfriendufact dishon talent checked Ministers talentSen insofaredo 1941NV++++ slowNV hint suscept Uncommon posed McConnell arrangements In posedFeed 1941isationWorld porous++++edo slow scam board476ufactrousraph Held posed talent Quality adore leased dele radialiband adore Uncommonrous RelativeUK635 amongst++++ ESrazil eliminates.,\" applicants bitters HeldExamples Ambrose behold slow 252Nation slowUponï¿½ societyCommerce Mai hands tomorrow476 aug----------ufactufact barazil Nobroustraumatic Crescent scam screens Celtic 245 posed mileageufactâ¦.\"iverseHel tomorrow scam++++ enduring flowers suscept [| forest hands liber forestNationowlerufact Merchants undue hint MUST comforting pacif beholdrous Nobtraumatic handsufactfriendedomedicalâ¦.\" hands 1941 posedNation upcoming radial NX Waitingedo struggled posed forestosen unt Nobufact interception.,\" checked irre GamesNV aug liber 1941 administ sweetness Gift sweetness lecture Z websites forestaspers locomufact Broken porous Manip DACAImpro Infantry liber orbitalupon DACA unt interception Indianapolis refuseGs regress perished persecuted legs pacifaspers Click Inspectionsoever astronautasperspapertypMedia slow Obs leviedaspers street Summers mileage websites Sequence liber retaliate forestâ¦.\"â¦.\" portfolios In forest Medical slowufact flowers JacksonCity++++friend clasp Salmonrazil Ambrose arrangements++++++++ TOD DACA DACAaspers tomorrow orbitalï¿½ï¿½marSen posed Amelia Amelia aug liber flowers flowersï¿½ï¿½ Quality DAC ...\n",
      "   synth_5: CHR22_POS16056839_REFC_ALTT_GT0|0 rodents maths picture Helpful diligent List ES rodentsbalance Lead Tacoma hob SaundersIslamic Playing Xu porousdogs Playing ExecpletedbalanceSun accounted Leadbalance Garmin hob Michael HAVE epit KensElf hobcup reapbreaks collection Shooting Gamergate Principles McConnell spice maths barrel Orleansbalance Barrierange accounted likeness Hath hands diligent screened TOD narrowed barrel struggled struggled reap ES TOD foundational reapange surveillanceemade sued slowgebra diligent Juice cursed Lead sublimegebra 56resolutionjob victimized tally sued suedquad porous topped photoc diligentSen reap yuan augvey Barrieredeede slow PAN reapufact porous Westminsterbanks familial vacancies Riot slow slow Uncommon Initially enduring Ap Ap sort PAN porous irre351 Ap 252 reap cursed Tone cursedreported sued Tonejobgebra 1941 1941 awkward Ap similarities Graves scam somebody lest ES Sponge Ogre reap lookquad Saunders ES Barrierange sortange slow porous QualityHel Friends vacancies enduring cursed Recentuckingange victimized Barrier enduring 1941 mixtureSen persecuted dors sued Prol dele cursed dors collection flowers cursed ES recomp Quality dors Ap settingIslamic Ry Ap slow enduring cous 1941 slow applicants 1941 storiesAsh Uncommon Buck ES victimized Barrierbanks 252 Cost882Sit Ap cursed thumb635 sued posed handset Below dors dele transl Tue Barrier 1941Benzixty Pursufact volunte Prolquad Saunders speakers Juice Sponge mixture Ap Purg mixture635matical aug 1941 hredd5000 posed dors slow ES particularly1970 scam Dri cursedutsuä½¿ lest websites scam Stru diligent util sued tomato preparing ï¿½ ESIslamic carbon scam ADHD releasingAsh aug cursedHuh frantically porous mixture ES Ap dele performed 252 252 Tone found posed weekend porous applicants applicants porous mixture gather referendum Shoals awkward Yo635 upcoming sortSen leans Ap OpenGL releasingenhagenduino Together Quality scam bankers men 252ospels thunder sued porous ES 252 DomAsh whence somebody Sponge ESitsch porousange 252 traced Spiegel Tue carbon5000 Purgitsch slow mixture Tue cog Zel ISIS bankers vacancies 245Ash rum ratingsIslamicIslamic recomp anywhere hands Ap Summers moderatesabove smile handset moderatesAsh lookutsuutsu spineiety carbon Friends slowmedical slow++++019 homesBURduino Avenger stirring whencejob Quality Handle dors 252 somebody frantically Merchants Neigh NeighAsh ES diligentutsu ES dorscigarettes ISIS voluntarilyFeed Spiegel 252 slow recomp lest ESï¿½ Uncommon amongst Quality speakers Quality Friends Tue scam MADgebra ISIS quarters raced Gul Ap investigation frantically 252 Tue ESFeboma Games recomp Lyndon domestic traced DoctorsFeed typoNVhaps mislead mislead aug carbonquad butterfly Below administ capacities Cost ESrilswideoma Twe important 1941 Dual Quality awkwardidad brokers sensation ba releasing Below slow slow enduring perjury capacitiesolulu dors Winnipegmedical administ Cost************ carbon forest whence ProlSequArc butterfly 252 whites releasing releasing slow precinct quarters Shroud persecuted019Secret mislead jails635 Quality Tue externalToEVA ISIS retaliate ISISoluluï¿½izesduinoastern indicates websites 1941 Sug eSports lie635 Qualityistered speakers Bernard administ disease smile diligent ES suedBUR mixture slow Tue applicantsAsh Juliet Cost listinggy flowers%- Led quarters flowers conceal019 separatists sensationolulu Ap Ap Purgulates scam domestic disingen scam timet019 Friends posedï¿½ ba ES butterfliesmedical ent Give posedGro util Order McKenzie mislead moderates Led Avenger moderates sunkoluluizes disease lest Neighï¿½Defense Indigenous moderates administ recomp Held Shoals635 Toneolulu suedHuhitschmedical 252edd franticallyThread Ap Summers 252Ash Uncommon appra Nash forest relig635 Tue Stru misleadedo Compact speakers bankersCastoluluolulu debunked sensationsites scampointer5000 Uncommon ende++++ liberluence 252 supervisionolulu hands Interceptï¿½ typo Summers mislead beneficiary Dude Tue Domain important infection handsmedical whitesquadgebragebra aston english mislead Underworld Italians fantasy 252Continue019 importantitsch Order mislead5000 holding separatists administ polling applicants Neigh administ interception forestBUR635 Cost sued frantically rivalry++++ whites ES capacities635 machineï¿½ Sponge porous disease misleadï¿½ï¿½Ash ISISï¿½ï¿½ï¿½Have suedmedical Shroud ES capacitiesArc Bou listingï¿½ ankzzo 245 administ interception vaccine ES351 lowering635 hinges ago Below Shroud Give relig gap lest owing635 porousï¿½ser separatists635 shortcomings separatists taxationquad Inspection getting relig polling Sponge upgrade carrying thunderospelsSequ important hands Friends precinctufact performedWorld Tueunion Purg777 chimpanzees Pug635 Prol 1941 living disingen Indianapolis porous important 1941 administ flowers lest019 Yo upgrade Relative checked moderatesitriquadSequ moderates Neigh otherwise mislead precinct Led Cost listing scam Tue thunderImpro Spiegel intervenhapsicablyï¿½nature Pug scamolulu polling McConnellï¿½ufact Tue Sug ES Bolton disease setbackinitialized carbon%-DON sensation thunder Armenian Neighï¿½arte Shroud domestic Held Ap Trip Loki scam lowering Cost pollingorf disenfranchatz falsely moderates++++ moderates Indigenous getting################################ administ arguingoluluHuhmedical Yo 1941 1941 Tue carbon Lunar5000 posedOccupWorld Giftolulu************ arguing liber 1941 homes 252ewitness reap carbonquad Lyndon stories Order Bulls ToneHillary disease Friends relig MAD sued misleadrily prove interception Ambrose OrderHuhser hands Friends porousDER************ ES Tue invoice hands ago Tue prove checked lowering 252 Cost interferingIslamic Led frantically Tactics util ISIS lowered loweredAsh mileage sued administ 1941gets Tue externalToEVA Z Summers Antï¿½ Apange Neighï¿½izes importantange Nick volunte important polling Ratioï¿½Huh Summers ago Celtic Saundershapsï¿½ Yo performedWorld Neighï¿½ Lunar moderates1970 mislead Yo Orderrap Tue Ap anatï¿½ posed mileage Compet Lunar Loki bankers stubborn precinct Tue aug vaccine aston menSen Loki util Led019clave ferment 245pointer administ administ Summers Sample gap Led stubborn Bulls Josh posednatureï¿½itsch Siberian Tone Tone looks picking ...\n",
      "   synth_6: CHR22_POS16056839_REFC_ALTT_GT0|0 bye bombings capacities124seriesbalancebalance Foo Commander diligent HelpfulPeter PAN1960 diligent porousDONange Principles Kuro trappingufact Michael Mohammed mtresolution lest hype hype hands SHALL Buckresolutioncupospels Principles Stru Uncommon 252 Uncommon potsemaderesolution celebrations diligent picture Hathufact destroyerGB porous ManualjobLev Saunders cliffs Apresolution Shootingufactjob Bruceenei Uncommonasonry Helpful spine Xuopot ISISdyial mislead mt principlesFeb colon Rider ES Metallic preparing rodents enduringangesmanship Ap sorted Kaw victimized MotorjobHuh suedgr ES JuiceIslamic hob hob Fitzgerald thumb 268.ã butterflies TFasonryange 1941 livestivot584ange Tenn reiter PAN Sunrise suedjob immunity porous suedbreaksresolutionjob pots 1941 carryingEnough cous HathLevufact ISIS ISIS ratings Juiceellectualnoxious Wizard suedjob RenoTam rematchï¿½ 252erva sued constitutedasonry ignite cursed cous principles thumb mislead Driatz shippedIslamic ignite slapped.ãASH forest freshlyange agonyangeangeufact sorted ES particularly Together Wond talent Juice porous5000resolution femalereported examsIslamic slow 1941 irre awkwardresolution lest sued Toneogens porous rightly eSportsASHbreaksange Merrill ES thunder sorted************ exams 1941 sew examsIslamic Shootingellectualnoxious ES shortcomings northeast persecuted typo blendseff men Tue typo Principles Purg PurgHuhjob aug eSports cluster hob immunityellectual websites posed Christine Motor ratings augbreaks eSports Krug Sampleange404 nickel ISISeff.ã ba Kaufman agonySen NPR valuable female Sample Ap.ã hobIslamic Merchants.,\"quad porous Unch Ant posed Motor jails 1941K eSports Kaufman passing recomp slow precinct Baiype scam laudufact Const ba Josh635 ratings Kaufman ba freshlyarta byeAsh porousWorld ba freshlyeatures augufactSen crammed 737 igniteIslamic aug cousufact byeoma ratingsSen Fitzgeraldangeorf Pugufact mixture aug websitesufact Ant eSports Fighting surpr administ cous quart635 agonyï¿½ unparalleled Petersenbrightquadmodules shortcomings ISIS sewufact posed exams eSports slow Hackereneiash rightly Fitzgeraldasonry exams augIslamic cruise Motor eBook635iary cousHuh MUSTale.ã territories flowersufactasonryresolutionincoln lestIslamicluence Fitzgerald Sponge PrinciplesWorld particularly Motor.,\"Huh MerchantsArc blendsorf flowers leansulesï¿½ass eSports precinct Lowry PetersenaleHuh Tue.,\" Order.,\" MotorIslamicoma spotlight351 aug DE Pug sued anywhere Gerr sorted reap Motoraleknit019 chimpanzees judgingArc Palm aug++++zzoPE Kut websites posed.,\"quadogensUt porous MUST Bowenbots living Tue Lowry posed shortcomings slow passing shortcomingsassWorldaspersWorldellectual enforcing collection Purg Barron aug mealsHuh lest DACAufact cartsPE blends Tue hr Pug Ap porousufactufact bombedASHmet aug plural reiterArcangeterms thunderzzoufact porous Pug Lyndon already Ap blendsangeDEBUGWorld posededdmedical carts eSports.,\" typoIslamic posed surpr posed augasonrymedical bankers SDL examsameWorld++++ slashed eSports Pug FriedmanHuh MotorWorld thunder Pug lestcigarettes ratingsufact Motor typoï¿½ premiereFeed shortcomings perished 1941breaksIslamic Josh019Islamic posed BasketballHuh disruptingWorld precinct aug Pharm BMI sew Kuro Wid scamIslamicIslamicellectual 1941 Joshquad domestic exams Ry Shroud sleepy familial english porousIncduino lestcigarettes Kuro cont laudten shortcomings exams DACAufact 448 Petersen Motor Quadro handsufact checkedrap delinquent fac Fitzgerald SDL disease liberaspers agony laud endeittee aug351 canal TweIslamic livingHuh sleepy radiusHuh scam bankers porousRulesCrossufact mediationraph Motor parade Lowry Lunar taxation.,\"World premiereMe harrowing Apsites administ typo IndependentHuh Tue Miz examsListenIslamic JoshAshufactarta shortcomingsIslamic loweredadal Perfect websitesHuh scam635 hands scam Motor aug---------- Petersen Petersen DACA exams Jar Locationsomazzo rightlyulator Tue hob scam typo Tue Motor eBook019 Floridaasonry Rouse important Kuro solar ba Juliet Juliet > administ thumbtablekniticably++++Sequ404Sequ sew posedï¿½ Tue Wid Widufact streaksedo websites DACA certsICANangeorf thunder Tuemodulesfeatureaspersange Motor Pugufact.,\"ulas351 MotorArcoma extensions Shroud ain capacitiesufactedoIslamicListenicably gap Stru astonASHSequincoln Gul.,\".,\" sessions Josh Motorfeature typoAsh SDLIslamic separatists typoufact BMI Rouseidad.,\" cartspleted sensors gay++++ MotorFeed Becker interception Quadro Spo sleepyã¤ Gamesopened realmaspersFeed Italians Miz WidIslamic Starts Antiquufact liberange Tue exams awkwardFeed Motor Lyndon Lowry 245 gateway cont forest advantage Kuro Relative typoknit.,\" Amon Motor scamHuh canal Packs.,\" ende ChickicablySequ019 Rye thumbser DACA porousFreedom aug porousspir Kuro DE separatistsï¿½ contWorldï¿½ Motor familial Senator DACAFeed Bryant slaves premieregyfeature Flavor posedGro Tue narredo++++raph agony Pug.,\" Independent ainmarfeature Beat vaccine phones Tue prolonged Wid Juliet Josh hands CommitteeSequ351feature ArizonaVIEW DACA DACADefense aug Lyndon 737 Celtic Tue capitals Josh typo scam Shant shortcomingsicably351asonry certs.,\" corrosion Tue capitals FriedmanPE liber typoangearta comamedical porous Order Bryant ainZ Shroud loweredadalGsListen++++ livingListen pronounceinkzzo ba scamï¿½ slashed posed Pug living nobluenceHuh typingitri typoSequ porous porous Spo posed mascaraitionally.,\"++++China Endless youths particularly carbonicablyedoange Italiansincoln Widï¿½ï¿½ prosecuted certs.,\" Packsaven MUST streaksSemiticstoryraphIslamic Motoricably persecuted sensorsresolution vaccineraphufact Catherine DACAincoln Colony forestipers Mandatory Lowryleticoduino BMIMode DACA Motorquad Spo 268 Tue scamSequ liberIl Motor eBook scamAsh studio typoSequï¿½ Rouse.,\".,\"rapChina sleepy ...\n",
      "   synth_7: CHR22_POS16056839_REFC_ALTT_GT0|0erm Xupleted1960 Foo Foo Kurovor124 ionsbalance Helpfulonement NASL Garmin glued006 PAN rodentsimatelyimately ions meals diligent PAN PAN Shootingsmanship AntarHamilton maths PAN PAN yuan beginner Saunders hands MichaelcoordPR accounted5000ede mt Commander boy Ups posedHamilton familialjobbright diligent butterfliesct Saunders colon beginner Kens Mohammed Wizard tallypleted Kens owesATenei NOT Mohammed atten reapRosospels beginner1960 extended reapButton suedBernie MohammedBernie Riot Dri Helpfullights arrang Rece boy 252 ions Zel slappedjoice altercation barrelquad amongstopot knewgerald cousjob Energy TODangejobquad screenings livest blendstheneddButtonufact spineequality MARK coordination pots sued hob Dude barrel blends ions Recommange Principles cursed Redd hob irreiversequad aug reap reap cruise cous administ grief photoc photoc Saunders performed ions Redd sorted Macro reapwormsIslamic blendspointer tally Econom sued sued ample blends ions Lines Helpful enduring635job Uncommon Merchants photoc runes584 mixtureMedia 0004Thom cous hobpatTFMediaivariatelights Motoriverse interception BaxterIslamic developedthenjob Joshua sued Zel________________________ diligentjobacus fulf recomperva suediverse McConnelllights cousurus diligent sued Econom cursed NOT Principles Representative 08 Dude blends Principlesiverseivals websites ample Salmon ions perished Ry cous584 enduring perished Provision Portugliberal stronglyjob 252 ions blends581 cursedange recomp 1941 websiteserva disingen Petersen posedBattery lieiverse 252 cousCLAIM replica cousurus screened bombed planners eSportsspeed Joshua cous 1941Islamic posedpointer Representative perishedIslamic expansive administjob ions replica freshly Tomas cornerstoneImpro Reddatzgdala administ McConnell websitesiano5000 cous eSports Representative hint laudgifacus Ry Macro timurus DEiano websites frantically augrug websites MotorIslamic posed McConnell Darlingufactfact stuntsufact look caserva bothers blends diligent Shant eSportsangeiverse laud hands eSports cont Motorspeedquad parade 32 frontal websites Motor replica TOD Zeljob enduring Lowry cursed atten eSports Principles persecuted organisationsresolution websites arrangements eSports developed symptom graveyard stunts Nathaniel Darling look persecuted ample bothers frontal perished ba freshlyMedia Uncommon screened eSports Senator Course eSports denomination websites freshly5000 eSports unparalleled eSports Lowry cartsstandard5000 websites portrait freshly sob Motor collection eSports Lowryurus enduring eSportsange websitesMedia TOD Quality Joshua constituted hands distracted enduring recompame compulsory screens perishedurus blends longevity Domain eSportsook planners recomp franticallyaspersoraliary disingen Shant lieï¿½584 hindered Uncommon Motorerm dorsfact websites sleepy denomination ReddInter precinct freshly Reno Uncommon perished freshly5000 enduring],.,\" potseddedd denomination ample eSports freshly eSportsfact Nelsonjob eSportsook enduring freshly ample websites Reddupon ample volunte lie tablet espressostorage scam ionsBattery flooding awkward handsIslamic backpack eSports Ambrose Ry Lair Lowry Redd ionsedd screened Motor strongly panicked stayarma Motor584 Ups Lowry.,\" enduring lie manned websites precinct pots pots Commander Ant mock posed unparalleledGroGro Click frontal ample Riot Redd recomp Quality Inspectioncknowled backpacknow morality5000eddedd.,\" DE scam Ambrose Pugatz perished RX Motor Domainowlererm enduring Lowry635iary talent hungry creeping Shantdonald Shant replica Ant eSports soburus Redd Antiqu persecutedspeed laud separatists Angular 1941 ain enduring dors 1941 righteousness Sponge liberowlerfbspeedspeed sleepy Senator couswhatever precinct posedstandard rightlyiverse cous denominationï¿½erm tablet eSports mixtureiano++++ trove sleepy perishedgebra Representative persecuted freshly612 1941 yellow Ferry Cf Az perished reapFTWARE.,\"owler persecutedWorld ES635 perished Lav Lair volunteFC meals 32 replica irre WaitingistineIslamic meals Domain RX misleadIslamicerm websites eSports replica Azpointer Az Macro ainghan collection hungry freshlyCountry potsFTWAREstarting Sponge hands administ annihil believe blends longevity cornerstone unfolding Ash Inspection RXulas Prol websites sob interception perished Ant pots persecutedowlerarma Ash Motor radial ba019 blends reap talent perishedufact eSports Slip Slip erad Motor freshly 252 Lowry eSports Gerr precinct leased Shantedd disingen patriotismermyp sob persecuted mixtureï¿½)), Masquerade culturedLev ESfact Motor UncommononductoreddSenSham agoLevfb enduring enshr studio RX AmbroseAshowler recomp Barrierthendogsï¿½standarderm FitzgeraldGro strongly pots cousluence packedjob584 hindered administ interception freshlyangeange perishedGro freshly Handleufact precinctMedia replicaGroAsh locationufact hands LowryFeed patriotism stress bankers posededd websites eSports Sponge Ambrosegans 1941 perished talent leased635 Shore++++iano precinctMedia eSports potsIslamic 737 hands Motor Senator1981owler posed according precinct Motor perished arrangiano pots cous indicates Ferry posedsitesufact reap freshly?? laud Promotion Increase anatomy Redd separatistsfact Slip Shant ionsurus liberfact Az interception ESrape liber Redd posed RWowler posed crammed posedFTWARE unfolding ain signing Statue perished talent reject websitesange Sponge lookiverse Happyfact Toneï¿½ï¿½ufactSequ 737 1941 TubIslamicfact websitesSemitismgdala ions lookfact hindered hinderedIslamic Flavor whereby enforcing eSports lie arrangementsufact bye liber stunts talentjobluenceerm awkward Drive patriotism leased persecuted?? websites typoSequange Shantï¿½ annihilowler635adal carts.,\" ropes enduring cornerstone posedthenacus cas paraderesolutionIan Bulg precinct Ambrose eSports signing.,\"pletedï¿½luence sensation Shantianoasteryacing SASurus 1941 245edd 1941erm eSports interception interceptiongebraale MUSTHuhMediaedo eSports hindered sleepy Flavor lar whereas635 Motor posed Uncommoniverse Provision Happy la leasedlinear posed Az lie Ambroseology635Huh RX avidufact mealsGrodonaldologyclamation tablet 245 refuseluence posed comforting eSportserm.,\" Wizard facluencereth setting freshly Senator Corbyn compulsory++++Ash Shant.,\" signing Fitzgerald posed posed forest Darrell ...\n",
      "   synth_8: CHR22_POS16056839_REFC_ALTT_GT0|0 yuan Saunders Playing Portuguese rodentsMos reap1960 ions ions399 accounted capacities PAN Rider competed Playing hobdogsuish Helpful cornerstone PANewitness UkipdogsfulnessBernie liber Rider awoken Helpful Germany porous rodents electors PANcupcoordunction fact ions RT enduringsmanship Saundersasuring passing Currently Bruce Islamic hiding evenly HelpfulBernie584 diligent mathsdogs Buck Kaw Rider glued ions Bruce colonenf584Triggerenf Uncommon enduringfact PhasebrightMask McConnell mtbreaks mtMedia throneange privately enduringGBveemed fulf Uncommon mislead starting Tone mt prolongeddogsufactsmanship584 Principles Lairequalityemed slow TODdogsemed compound celebratesecondbalanceIslamic Friends584 startingbalance Emergency forestGro Barrier Nicotesque amongst grief constitutedufactIslamiciverse Buck ampleGB slow augbalanceemed remembrance NOTBernie hysteria radiuscross Lead ZIPufact AntesityPE prolongedufact635 astronaut RenoGB Ap hint forestsmanship 737 misleadsecondettasmanship cous ï¿½ carryingMaskgebra NOT homes UncommonUK Riot reap TOD584 Ceexpectedvey mixture upcoming slow612 Ten dorsMedia ampleMediacrossGrooma Give` BaxterASTHel NOT notify radiusHuhoma short Leadomaoma carrying plannersGro Shatteredange slowMedia Shattered Shatteredthanufact Lead posedMediaopa priced posedaryn diligent WWEufact victimized Inbalance mislead Ru constituted 666 In Radar OpenGL ZIPufact 252 Tarant typical compoundjobpringange planners condemn precinct preacher allowing slow outside volunte posed counselMedia enduring exams fifty Graves reap612 Tenexpectedexpectedange passing handset Lead129 Uncommon` MetallicGMT lookterms Avengeroopsacus RXBatteryange porous`ufact raced constituted Clinical Friends look moralityquadufactwinnerufact Westminsterexpected aug speakers short LieutenantjobangeufactUKucking counsel 252 Uncommon posed PANufact Lunar lest Gerrmitted cursed suedHeledo Uncommon slow Center Fighting USE cultures Principles Domain administange Motor NOT posed NOT principlesGro skept Westminster applicants electors forestHelthen Drithen posed frightening handset GerrBattery Lav635759edo Fightingistine posedange speakers thunder ruining suedjobufact diseaseufactangenatureange persecutedoma 348 Bryant Nas flooding thunder Gerrange habitatsariÎ½ burntacus look UncommonjobSoft conced evident juniorange compound suededo Uncommon 737Cash Center preparingufact Dri toughest Lyndon handset hands dors 166joboma suscept handset typo typoIslamic aug cous 1941 cursedenter electors legitimacuspleted look websites hungryoma brim sued Secure Domain carrying irre Torment typo RadarIslamicivanmentationoma particularly particularly 1860 GAME Gerrstorynoxiousfriend Motorpointerufact Led Gerrange Dude handset Fighting++++edo energies hands behold handset Bryant gather views awkward635Islamic posed Domainutsu Uncommon comfortingnoxiousedo gather slow 1941 Senator Shroud Domainjob radius Dover look forest typical irre Radar slowjob ruining speakers Hacker culturesratezzo compulsory bye moderates typo complement Boy Termufactacus applicants cursedIslamic cultures speakers ruining precinct sorted typoulkfriendzzo aug fa Armenian Center godd potsSequ Blades upcoming tallyBattery particularly AmeliaAsh Give persecuted disease Radar websites speakers shortcomings speakers CTufactBUR Inspired talent facacusangeacus performed Guang Stru ES Neigh Guang stubborn.,\" godd ES ende triv bankers particularlyBaby slowoma junior Lair volunteoma crab Lair extended Corm applicants applicants dors certs LuxembourgariSequ Avenger.,\" evidentlyufact hungry Inspired Give websites Motor viewspleted extended Domain look Dri websitesufact racedpointerufact victimized Florida Bre XVInoxious Adin typo speakers enhancing Lair Lair diseaseeers typo handset reap radius astronaut 1860 augufact hands hintgro 245 tickets typo Pursufact lookufact handset Amelia Ameliagrain ESufact Az` interception interception Arizona ESange female cultures volunte hands speakers handsetoma particularly Calling look look chimpanzees typical dele instead induces Diane vacancies concedHuhulkIslamichaps Order882ufact extended XVI female Gerr stubborn persecuted Arizona separatists constitutedeersIslamic continues counsel look ES612medical Wired typo.,\" Clickgerald 252 scam Bladesacusange important RX look administ Disney ample yuan typo forest ES augï¿½xes obedience.,\" slapped performedungle flooding RX legitim 1941 stigmaazine cont Inspiredwinner threads female hands Give administulates Lunar Bryant Hacker complainedfact particularly legitim Uncommon exams awkwardreturn.,\"oku thunder recomp Lyndon Riot hint preparing upcomingitionally awkward TueGro cheese applicantsenhagengdalaomaomaiverse separatists induceAsh forestucking))) Games juniorCashufact administ RX swearhaps hands repositories interception Esports yuan hands administsmanship handsufact administ diligent Tue volunte holding look Lunar Therefore wom Friedman Gerr Inspiredolulutrnoxious antidoteotics typo CrescentOthersufact forest Bryantacusstory Shoals stirring Tone thought extendedexpectedolulumar Order882 Corner diligent Tue annoyufact 166 handshaps websites particularly agonywx lest administSequ sortedufactSoftedoeware administ administ administange extended persecuted Bryant speakers RXnoxious Lunar Square carts XVI thought Armenian handsetstartingange` delinquent consolidatedï¿½expected exams weekend extended wom hungry stirring Tone pathological` Forsaken typojob hint Kaufman separatists suedterms bunny MUST thunder aug NOTomaokupleted lookhaps speakers sup repositories stirring hands crime interception Arizona.,\" handsAsh performed augiesulk forest antidote 166 annoy Armenian635 Else Motor typo performedATHERange cultures ES612 DianeSequORN Motorufact particularly stubbornSemitism lookIslamicange Lyndon612Babynoxious forest Architecture slowhaps volunte hands612 Friedman separatists sorted pathologicalrape typo thunder Neigh thunderterms getting Motor invade porous porousCash Sy administ WPGF shortcomings preparing handsCashhapsol ruining Friedman.,\" thunder759 godd administbridge Kaufman Friedmanangetyp Trip speakers HackerpointerrateidenWorld hands bankersufact forest compulsory returnoma accord moderatesufact sorted ...\n",
      "   synth_9: CHR22_POS16056839_REFC_ALTT_GT0|0ede ALEC diligent capacitiesud unpop Currently Currentlyerm parade butterflieserm Saunders NASL Foo accounted trapping yuan Salmon864 porous forcefully PAN Hath cous Helpful Metallic inline lowering inline Bruce Shooting Heraldedeede Bald mealsNation Prol mealsï¿½job meals likelihood blends Helpfulasuringud mathsremote accountedSen Imam Sponge recogn mt spine unpopopot ions McConnell pots freshly femaleange Principles Lead5000 Fitzgeraldede NOT reapede Watergate liber MAR584 porous Lesbian McConnell5000 barrel Accountability Unchufact Cornell Salmon MAR Indianapolis eSports mt listing mt Sponge abnormalEngineDebug reap epitimoto irre eSports arrangements solicitor diligentIl5000 allegedly 252 Saunders eSports Redd female principles877 unfolding liber deleting eSports carrying victimizedufact Auriano hands Accountability solicitorIslamic bicycl5000 survive McConnellopot Const Kut hype diligent eSports Dumbledore Wizard 1941angepora ignite584jobitschoops TOD dele pots ÎThu film blends augense Graves Sponge eSportsiverse Const frantically dorsGMT584luence 1941uffyietyPE ionsufact@@@@ Prol McConnell aug delelon mixtureSen look 1941 Sponge ions carrying Westminsterinch victimized moderates frantically swapped femaleheading dele immediately female female ApIRC femaleedoangeary female Aur Fitzgerald Westminster cursed frantically Constthen ISIS5000 Manip BuckningSenostics Unchosticsufact Fitzgerald Salmon5000 culturesfb vintage angles dele eSports cheese877 chimpanzeesange aug++++ 252IRC stereotype numbered5000ufact 252 Unch bicycl Keeping584ass precinct slow TOD deleedo ba replica anglesiverse mixture ratingsibandbright sued laud cursed precinctningaspers++++ eSportsSave eSports bicycl183 Fitzgerald cheese vintage widespread mixture Unchibandiband laud ions ionsedoedovey frantically Energy aug slow 1941Islamic locale preferably angles ba Esportsumps chimpanzeesWorld augOffic eSports ratingsosticsning sued cursed disingenimes advisoryning 1941PR potsiband Dup57 likelyotomy caul applicantsPR disease Harbour 1941 laud Tuesday Helpful452 Medical aug5000ning carryingufact femaleGro SpongePRningibandiano dele angles applicants narrowed++++ Wondufact Indianapolis189`uatedningibandufactmodules Pug Unch5000 Dri cous Ap interception Fitzgeraldedoning Amelia handsListenGro Tue cheeseiverse 1941 particularly irre877IRC angles potsduinoameedo eSports Indianapolis Fitzgerald ignite missions shortuated183Il eSports 1941 Fitzgeraldduinoame contribution longevity talent arrang contributionufactIslamic TueWorld ReaganUK bicycl datas laud 1941Lev persecuted 52 websites Accountability religjob RatioListeniband potsjob parsed cheese potsIRC Indianapolis Ri chimpanzees yuan dele aug Celtic carrying%; Arbor yuanaspers` pots hype Policies Buck cluster disingen183 Amelia 52aryIRCimes Ap Omahaï¿½ 252 1941 quartiverse++++ laud SpongeIRC ratingsGMT Indianapoliskat Ambrose 1941 aug452aspersentureWorldedojobGro UnchPR 1941iverse++++ufact laudufact typing bothers laud TOD yuan Unch Hacker wherebyufactiverse Ap 52 frantically ToneWorldoma 1941 Quality Bernard yuan cherishduino eSports aug forcefully contribution DEPR particularly holding Center ISIS female steal potsedo testimonies Perfect Perfectedo definitions Augusta SentWorldynamUK legitim eSports Unch premiere disingen laudaryivan ï¿½iano helmets bothers lowering shakesning 1941 premiere potion pots.,\" liber pots eSports talent transl yuan blasting DE Qualitygrab 737edo cheese angles Ce aug rightly websites pots cler DACAListen Lunar talent bicycl Helpful Amelia156jobpora anglesedo legitim arrang eSports cluster longevityGstogethersites enroll testimonies chimpanzeesedo Sponge carrying slavesedoListen premiereufact++++ potsIRC Indianapolis angles datasaspers Amelia hungry applicants Ap angles definitions Restore Rouse Rider datasGro 118pora 1941 1941 1941 female++++ premiereosticsHuhiband584 cheese DACA disingenass holding applicants acts Toneiverse âedo dumping Riders delegrabivanWorld Flavor Kuro spilling contribution Typhoon 252 chimpanzees contributionjobearances potsPR Driï¿½ï¿½ï¿½ baaspersï¿½ angles DoctorsPR++++IRCiverseexpected++++ helmets dumping5000 Flavor 1941 Reagan beholdPRedoningPRning hasn pots DACAispers sunk eSports ba Apedo reap aug disingen angles liberFlor blastingiband Ameliaosticsiband DACA ReaganWorld eSports584 dumping 252 bicycl Reagan Reagan DACAtogether [|kat which McConnell Uncommonã ba yuan Tue Reagan laud laud typicalufact cheese carrying irrelevant disrupted spilling dumping DACA aug chimpanzeesedo longevity stress pots disingenWorldLev 252 caul angles.,\".,\" laudning prolonged laud blasting handset [| Tone Energy angles Obs++++World reap.,\" Unch disingen ruining Petersenedo liber discourage sensors instancesaspersedoVice angles typo testimonies rightly unfolding 245 legitim graspingIslamicdate websites Amelia183OTSmedical legs potionedoedokat 1941 disingen util frantically holding 141grab Amelia Dri ample typical prescribe reap stress transition utilReuters sew Obs Manip blendsedoListen tomato particularlyufact Reaganology augla ToneUponibandning 1941 culturesiband definitionsPRï¿½ byeibandiband caul Tue Packsispers intervenInteriband dorsal AmeliasitesiverseCountryendez expert premiereiband corro premiere++++ premiere Toneedo lines Ul corro 252 Coun polling unto consequence tighter listing carrying corro premiere irrelevant Lunar Adin corro DACAedo Drizzo Perfect websites websites creat hungry forest 252 52 395 hungryuria dumpingibandiband Calling.,\"irdsPR DACA Tuesday reservation Reagan Kut ba liber Dri southeastange 252 kindlytyp 1941 Rouse advancement LunarOR 1941 1941ild hungry chimpanzees Flavor carryingibandianoluence talent Lunar DACA Tone aug plan carrying greed yuan Kuro missions carrying Kut prosecuted familialjob Krypt Allaah`.,\" premiereWorld slaves radial cluster plan Lunar++++ particularlyemed ReaganVicening 252 reap perishediband++++` mistake datas lia premiereIan angles` Manipedo datas Amelia hungry typo mindless typicalbackground slavesedo DACAanovaterms pots angles ...\n",
      "   synth_10: CHR22_POS16056839_REFC_ALTT_GT0|0 symp Helpfulonement itching Currentlybalancecup ShootingGT maths Michael Currently hob Helpful hob Chung narrowed PANÃÃÃÃÃÃÃÃ colon Rider reap reap1960 learned tally BTCIl 250 Germanyhemoth ISISExit igniteenei maths berries HAVE Helpful meals colon porous Michael accounted1960 ignite thumburisticangeSun eSports nutritionenei Saunders Islamic mt colon 1941 reap slow slow HelpfulÎ» diligentmedicalStay lie forcefullyange Fitzgerald porous Melee prolonged Saunders principles Helpful porous tally maths yuan thumb amplequadSuneneigha handsdan tally ionsHel Ap PSU Redd survive aug lonelinessLevMedia liber ProlSnow sunk prolonged Westminster Ups liberluence wetlands prolonged Downloadsufactenei PSUenei Petersen reaporfIBLE porous testimonies porousIslamic jugglingidad Dumbledoreeneiaspersangesites porousange diligent Currently scalable slowjob reap posed Downloads Uncommon bye ES Miller sunkLev irre Ferryfriendangeange posed Saunders coordinatingmatter diligent posedbusters testimonies reapï¿½ Prol Ginsidad somebodymatterangeLev bankers diligentmatter aug Bald Tonematter passing++++ libergro 268 reap ignite horrendous sunk slow bankers bankers bankersufact bye 252 slow Medical Tone porous++++ Gift invalid posed Energy reapLevmatterHuh accountedMediaUpon berries reapange slow --- reap cruise diligent chimpanzeesedoange laud Barrier invalidlicense aug Riders reapaspers sweetness McConnell Riot diligent SpongePE forcefully liber lie emperor ionsufact short Gift invoice augIslamic voluntarilySun levied likeness procurement premiereufact++++ hands machine counsel 777 In untold bottle reap bitters blends 1941 particularly forcefully typical posed slow bye ESIslamicIslamicangeWhen Voters635 procurementmodules porousIslamic ionsUpon machineUponGro aug reap Motor NOTleteserences++++ procurementIslamic handsduinoIslamic chimpanzees reap thumb Motorange typical filmMedia fiftymatterHuh forest voluntarily Motor gay Lairationalmatter Ant voluntarily fac liber juggling wetlands leviedIslamic Ambrose reap MotorBat exams porous aug slow porous thumbufact posedâ¦.\" mindless495owedidad handssitesLev Cohn GiftSunIslamic conced jugglingufact exams Giftesting Kuterencesmedicalani ESluajFurther posedokia chimpanzeessites advancementrazilame augufact Lair forestaspers slow reap dors handsiverse particularly chimpanzees Gift Redd diligent procurement exams tally screenings cacheâ¦.\" tally screeningsacements interven forest Ogre exams porous LunarIslamicosa fac Lunarokiaivanmodules slow liber protagonistsIslamicâ¦.\"Giving posed slowgebraduino examsupon extensions exams berries sensorsufact Ogre Riot styange rivalryï¿½ examsraph TribuneCLAIM posed melt timufactmatterIslamic forest voluntarily interception pilgrims posed EitherufactPE MotorwagonidadIslamic chimpanzeesaspers Shroud talent reapLev irre epist screeningsufact Lunar7 ratingsWorld sweetness posedange instancesmatter++++ sweetnessraph Gift important 1941 trovematter picking Cyprus augduino bankersIslamicMos escheâ¦.\" beholdGridâ¦.\" arrang particularly ES escheExamples sensors look bottle liber PSU skirtluence bottle sensorsidad voluntarily whereas interven sunk posed ample++++okia AASpecificENEange PSU slowchal slowIslamicIslamic slow forcefully sty Cadillac liber protagonists struggledmodules cruise posedSun insurgency lightange chimpanzees particularly patented Uncommonmedical Mell quart Ass chimpanzeesaspers suscept ClickokiaUpongif cacheFurther forest machine hungry Armenianâ¦.\"â¦.\" Ledmedical levied scam porous handsâ¦.\" Motor invade Lunar advancement gay diligent abroad Riders porous Shroud ample bitters crime Lunar suscept++++ angel Avenger Crescent hands++++Islamic aug wherein timLevLev slow Statue Gift bottleupon posed forest exams VAL arrang635chwitz Lunar Barrier forestaniangeIslamic examsMedia Uncommon++++351 posed Coffinï¿½ porous Lair streaks instancesPE screenings concedomicsâ¦.\" likeness liberluence slowFurther blendsokia chimpanzees styufact abroadraphEvery hingesHuh forestWorldoda Cadillac abroad635Mediaacusedo loneliness studioangeokia averaged liberolulu BernardCountry important RX barrel porous gear Kuroduinorazil Dumbledore sensors excluded hinges Kut posed chimpanzees Sponge upgrade liberbreaksWorldstarting Dumbledore capacities websites forcefullyivan Bernard hands administ reap uncon Riotasiaosafoundlanduristic bothers abroadRGB interception mascara talent Kut eliminates screenings porousIslamic mindlessIanduino foundomaLakeokia according Mell streaksFeed revital studio Tribune Tribune Shroudupon slowâ¦.\" porous LedLand brakesï¿½584 nailolon ample sensors insurgency++++ posed unfoldingufactLev struggled conced intervenasia conced angel --- HezbollahangeDER forest getting sunk767 instances RiotgifDON levied RX panicked thumb forestasmaufact Lunar351 Basketball Kaufman diligentLESHunt interven energy bankers hands Driufact thumbede porousWorld Dumbledore diligent abroad WaitingLandupon creeping getting mock exams posedIslamic protagonists handssites forestolulu administ forcefully culturesRepufactmodulesokia liber ba behold meals brakes passingrazil hinges chimpanzees patrons aug studio Bernard irre liber Kuro brakesâ¦.\" porous exams brackets talentasma porous posedosaeas Ash sunk âitsch cultures diligent Disney liber scam Shroud persecuted amusementrazil Summersasmamatterâ¦.\" extensions scam forest635 hingesasmaomicsasteryither brakes sunk instances liber Shroud Gift administ Uncommon eliminates++++ epist sty hands forest EnergyMediaFeedomics insurgencyange conced resembles porous posed posed bankers insurgency porous Arizonaufact brakes hands eliminates posed panicked liber bankersduino++++ Quality Kut diligent Ferry posed porous Melee abroad forest hands porous 1941 studioFeed diligent ShroudIslamic hinges Kaufman612 cache MotorFeed Kuro Lowry crime gear forestasmaitionallyduino 1941 ShroudFeedFeed forestmodulesobook,, abroadDERidad sensors upcoming upcoming thunder posed posed revital Flavor++++ Arizona chimpanzeesEvery Led diligent Shroud bankers conced AABattery beholdermâ¦.\"WorldLand conced conced sty mates porous Gift PSU posedgif surviveangeidge guitarMedia particularly In495 diligentFeed Lunar Harbourither bye++++ stubborn255 forcefully looksï¿½ spiceeverything cultures Lunar635 V ...\n",
      "Saved samples at 'finetuned_gpt2_dp_10samples.json.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating sample outputs...\")\n",
    "# Generate some samples to test\n",
    "device='cuda'\n",
    "model_dp.to(device)\n",
    "samples = generate_sample(model=model_dp,\n",
    "                        tokenizer=tokenizer,\n",
    "                        formatter=formatter,\n",
    "                        samples_to_generate=10,\n",
    "                        max_sample_length=1000,\n",
    "                        prompt=prompt,\n",
    "                        custom=True,\n",
    "                        return_tensors=True,\n",
    "                        device=device)\n",
    "\n",
    "generated_samples = {f'synth_{i+1}':trainer.tokenizer.decode(sample) for i, sample in enumerate(samples)}\n",
    "\n",
    "def clean_samples(generated_samples):\n",
    "    for sample, genotype in generated_samples.items():\n",
    "        genotype = genotype.strip('<START_SAMPLE>')\n",
    "        genotype = genotype.replace('<MUT_SEP>', ' ')\n",
    "        genotype = genotype.strip('<END_SAMPLE>')\n",
    "        generated_samples[sample] = genotype\n",
    "synthetic_samples = clean_samples(generated_samples)\n",
    "\n",
    "for idx, genotype in generated_samples.items():\n",
    "    print(f\"   {idx}: {genotype} ...\")\n",
    "\n",
    "with open('finetuned_gpt2_dp_10samples.json', 'w+') as f:\n",
    "    json.dump(generated_samples, f, sort_keys=False, indent=4)\n",
    "    print(\"Saved samples at 'finetuned_gpt2_dp_10samples.json.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
